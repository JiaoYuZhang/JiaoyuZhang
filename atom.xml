<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Joey&#39;s Notes</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jiaoyuzhang.github.io/"/>
  <updated>2019-05-19T14:57:36.519Z</updated>
  <id>https://jiaoyuzhang.github.io/</id>
  
  <author>
    <name>Joey Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读-RandWiredNN</title>
    <link href="https://jiaoyuzhang.github.io/2019/05/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-RandWiredNN/"/>
    <id>https://jiaoyuzhang.github.io/2019/05/12/论文阅读-RandWiredNN/</id>
    <published>2019-05-12T05:38:15.000Z</published>
    <updated>2019-05-19T14:57:36.519Z</updated>
    
    <content type="html"><![CDATA[<p>论文： Exploring Randomly Wired Neural Networks for Image Recognition 【<a href="https://arxiv.org/abs/1904.01569" target="_blank" rel="noopener">pdf</a>】</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie%2C+S" target="_blank" rel="noopener">Saining Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kirillov%2C+A" target="_blank" rel="noopener">Alexander Kirillov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" target="_blank" rel="noopener">Ross Girshick</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+K" target="_blank" rel="noopener">Kaiming He</a></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/62837029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62837029</a></p><p>本文阅读有大量参考上述参考文章，也会补充部分个人细节与思考。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>RAndWireNN 基本思想是研究设计<strong>stochastic network generator</strong>，也就是设计网络构架的机制，它的关注点在网络的连接方式上。下图是本文中获得的网络结构：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g36xbcy5egj30h70hfgpz.jpg" alt></p><p>论文作者引入了一种网络模型空间的构造方法，即图论中的random graph，之后用grid search搜索出较好的神经网络子集，并在ImageNet的1000-class分类任务上进行验证。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>论文的主要工作包含以下步骤：</p><p>（1）基于图论的随机图方法生成随机图Random Graph；</p><p>（2）将Random Graph转换为一个神经网络NN；</p><p>（3）将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN；</p><p>（4）在ImageNet 1000-class任务上验证RandWireNN的表现；</p><p>（5）重复（1）到（4）步骤。</p><h3 id="一、基于图论的随机图方法生成随机图Random-Graph"><a href="#一、基于图论的随机图方法生成随机图Random-Graph" class="headerlink" title="一、基于图论的随机图方法生成随机图Random Graph"></a>一、基于图论的随机图方法生成随机图Random Graph</h3><p>作者引入了三种随机图生成方法，即Erdos-Renyi（ER）、Barabasi-Albert（BA）和 Watts-Strogatz（WS）。这三个方法生成随机图的机制比较简单。</p><ul><li><p>ER: N个节点，节点两两之间以P的概率有一条边。该方法包含一个参数P，故以ER(P)表示。</p></li><li><p>BA: 初始有M个节点（M&lt;N），每增加1个新节点的时候，该节点以一定的概率与已有的所有节点相连（这个概率与已有节点的度有关），重复，直到这个新节点有M条边。重复，直到整个图有N个节点。该方法包含一个参数M，故以BA(M)表示。</p></li><li><p>WS: 所有N个节点排成一个圈，每个节点与两边邻近的K/2个节点相连。之后按照顺时针方向遍历每一个节点，与当前节点相连的某一条边以概率P与其他某个节点相连。重复K/2次。该方法包含2个参数K和P，故以WS(K,P)表示。</p></li></ul><p>其中模型的节点总数N由论文作者根据网络复杂度（FLOPs）手动指定，因此ER方法的参数搜索空间为P∈[0.0,1.0]，BA方法的参数搜索空间为M∈[1,N]，WS方法的参数搜索空间为K∈[1,N-1] x P∈[0.0,1.0]。图1是三种方法生成的随机图。</p><p><img src="/Users/vera/Library/Application Support/typora-user-images/image-20190519210043114.png" alt="image-20190519210043114"></p><h3 id="二、把生成的随机图Random-Graph转换为一个神经网络NN"><a href="#二、把生成的随机图Random-Graph转换为一个神经网络NN" class="headerlink" title="二、把生成的随机图Random Graph转换为一个神经网络NN"></a>二、把生成的随机图Random Graph转换为一个神经网络NN</h3><p><strong>将随机图转化为DAG</strong></p><p>首先要给每条边指定一个方向，即把生成的随机图Random Graph转换为有向无环图DAG。方法就是给每个节点分配一个索引index（从1到N），若两个节点之间有边，则边的方向从小索引节点到大索引节点。其中ER方法按照随机的方式给N个节点分配索引；BA方法给初始的M个节点分配索引1~M，之后每增加一个节点，其索引加1；WS方法则按照顺时针方向从小到大给N个节点分配索引。</p><p><strong>为DAG的边和节点赋上操作</strong></p><p>边操作：本文中假设每条边都代表着数据流，将一个tensor从一个节点送到另一个节点。</p><p>节点操作：每一个节点代表一个一个实际操作。这个操作可以细分为以下三步：</p><ul><li><p>Aggregation：（输入操作：聚合）输入数据(从一条或多条边)到节点通过加权求和来聚合在一起；其中权重是正向可学习的。</p></li><li><p>Transformation：（转换操作：卷积）聚合数据由定义为[relu-convolu- bn]三元组的转换处理。所有节点都使用相同类型的卷积，默认情况下为3×3可分卷积。</p></li><li><p>Distribution：（输出操作：复制）节点的输出边缘发送转换后的数据的相同副本。</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g36y6tx0x5j304p05yjrg.jpg" alt></p><p>这样设计操作带来的一些特点：</p><ol><li>Additive aggregation能够维持输入输出channel数不变，防止后面的卷积计算越来越大，来避免仅仅因为增加计算量而提高大型输入节点的重要性程度，而忽略网络连接的作用。</li><li>Transformation应具有相同数量的输出和输入通道(除非切换阶段），以确保转换后的数据可以与来自任何其他节点的数据相结合。固定通道计数之后，不管输入和输出的程度如何，都会保持每个节点的FLOPs(浮点操作)和参数计数不变。</li><li>不论输入和输出的程度如何，聚集和分布几乎没有参数(加权求和的参数数目很少)。此外，假设每条边都是无参数的，这样一来，<strong>图的FLOPs和参数数量与节点的数量大致成正比，且几乎与边的数量无关</strong>。</li></ol><p>这些属性几乎将FLOPs和参数计数与网络连接解耦，例如，在随机网络实例或不同生成器之间，FLOPs的偏差通常为±2%。这可以在不增加/减少模型复杂性的情况下比较不同的图。因此，任务性能的差异反映了连接模式的属性。</p><p><strong>设置cell的输入节点和输出节点</strong></p><p>给DAG指定唯一的输入节点（input）和唯一的输出节点(output)。DAG本身包含N个节点，那么额外指定一个输入节点与DAG中所有入度为0的节点相连，其负责将输入的图片转发出去；再额外指定一个输出节点与DAG中所有出度为0的节点相连，该节点进行均值计算后将结果输出。</p><h3 id="三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN"><a href="#三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN" class="headerlink" title="三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN"></a>三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN</h3><p>借鉴其他经典深度学习神经网络，作者将多个NN堆叠起来形成最终的随机连接神经网络RandWireNN。图3为一个示例，其包含共5个stages，其中stage1是一个卷积层，stage2可以是一个卷积层或者是一个NN，stage3、stage4和stage5均为NN。不同stage之间：卷积操作的stride为2，故feature map的大小逐渐减半；卷积操作的卷积核的数量x2，故feature map的通道数(即C)也x2。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g36yof1qjrj30hx0puwo8.jpg" alt></p><p>其中每个NN的节点数N（不包含输入和输出节点）设置为32，通道数C设置为78或者109/154。不同的（N,C）对应不同的网络复杂度（FLOPs），这样可以跟不同规模（small/regular/larger）的其他经典网络进行实验比较。</p><p><img src="/Users/vera/Library/Application Support/typora-user-images/image-20190519214558824.png" alt="image-20190519214558824"></p><h3 id="四、在ImageNet-1000-class任务上验证RandWireNN的表现"><a href="#四、在ImageNet-1000-class任务上验证RandWireNN的表现" class="headerlink" title="四、在ImageNet 1000-class任务上验证RandWireNN的表现"></a>四、在ImageNet 1000-class任务上验证RandWireNN的表现</h3><p>（1）三个随机图生成方法（ER，BA，WS）的比较</p><p>如图4所示，WS方法的表现最好，于是接下来作者挑选了WS（4,0.75）与其他网络进行比较。论文还进行了另外一个网络结构鲁棒性测试的实验，即将网络中的某个节点或者某条边随机去掉，然后测试网络的表现，最后发现WS的结构最不稳定。这个研究点很有意思，也许将来会有一种网络生成器能够生成又好又鲁棒的网络结构，类似大脑，也许有种大脑结构最不容易得精神疾病。</p><p><img src="/Users/vera/Library/Application Support/typora-user-images/image-20190519214955554.png" alt="image-20190519214955554"></p><p>（2）WS（4,0.75）与其他网络的比较</p><p>首先是与小规模网络(smaller)进行比较，此时N=32，C=78。如表1所示，比MobileNet/ShuffleNet/NASNet/PNAS/DARTS等表现好，比Amoeba-C略差。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g36ypp9qr0j30e10b30uq.jpg" alt></p><p>其次与中等规模的网络（regular）进行比较，此时N=32，C=109/154，如表2所示，比ResNet-50/101和ResNeXt-50/101表现好。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g36yq88vmtj30e606pmyc.jpg" alt></p><p>最后与大规模网络（larger）进行比较，此时直接使用刚才训练好的中等规模网络，不过输入图像的size由224x224提高到320x320，如表3所示，虽然比其他网络表现略差，但是计算量少了很多。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g36yqmat6aj30dv06jab9.jpg" alt></p><p>（3）迁移表现</p><p>论文将RandWireNN作为骨干网络，用Faster R-CNN with FPN来检测目标（COCO object detection），如表4所示，比ResNet-50和ResNeXt-50效果好。</p><p><img src="/Users/vera/Library/Application Support/typora-user-images/image-20190519214922429.png" alt="image-20190519214922429"></p><h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文贡献：</p><ul><li>使用 WS 模型的最佳生成器生成的多个网络性能优于或可与完全手工设计的同类网络和通过各种神经结构搜索方法找到的网络相媲美。</li><li>还观察到，对于同一生成器生成的不同随机网络，精度的方差较低，但不同生成器之间存在明显的精度差距。这些观察结果表明，网络生成器的设计很重要。</li><li>最后，工作表明，从设计单个网络到设计网络生成器的新过渡是可能的，类似于如何从设计特征过渡到设计学习特征的网络。</li></ul><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ol><li>本文还是保持了相当部分的手工设计，对于随机图参数的选择方式是否可以改进</li><li>比较有意义的是，本文把NAS领域的重点从运算操作的选择转向了网络连接(拓扑)的构造。</li></ol><p>参考文章中俞一鹏博士的思考：</p><p>（1）这篇论文将图论的方法引入，并强调network generator的重要性，很有新意，好比“虎父无犬子”，“龙生龙，凤生凤”。</p><p>（2）这个idea也许你也能想到，不过快速实现这个idea的能力更重要；</p><p>（3）不同stage之间只有一个node连接，这个设计未必合理；</p><p>（4）迁移学习表现比较的实验过于简单，且其他实验结果也没有很惊艳。</p><p>（5）引入的图论方法有点简单，也许有更复杂的图生成方法，比如Bio-inspired类的；</p><p>（6）只与NASNet进行了表现比较，没有与其他AutoML方法进行对比。</p>]]></content>
    
    <summary type="html">
    
      RAndWireNN 基本思想是研究设计stochastic network generator，也就是设计网络构架的机制，它的关注点在网络的连接方式上。论文作者引入了一种网络模型空间的构造方法，即图论中的random graph，之后用grid search搜索出较好的神经网络子集，并在ImageNet的1000-class分类任务上进行验证。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="RandWiredNN" scheme="https://jiaoyuzhang.github.io/tags/RandWiredNN/"/>
    
      <category term="stochastic network generator" scheme="https://jiaoyuzhang.github.io/tags/stochastic-network-generator/"/>
    
      <category term="random graph" scheme="https://jiaoyuzhang.github.io/tags/random-graph/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-ENAS</title>
    <link href="https://jiaoyuzhang.github.io/2019/05/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ENAS/"/>
    <id>https://jiaoyuzhang.github.io/2019/05/12/论文阅读-ENAS/</id>
    <published>2019-05-12T05:37:48.000Z</published>
    <updated>2019-05-19T14:08:56.587Z</updated>
    
    <content type="html"><![CDATA[<p>论文：Efficient Neural Architecture Search via Parameter Sharing  【<a href="https://arxiv.org/abs/1802.03268" target="_blank" rel="noopener">pdf</a>】【<a href="https://github.com/melodyguan/enas" target="_blank" rel="noopener">code-tf</a>】【<a href="https://github.com/carpedm20/ENAS-pytorch" target="_blank" rel="noopener">code-python</a>】</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pham%2C+H" target="_blank" rel="noopener">Hieu Pham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guan%2C+M+Y" target="_blank" rel="noopener">Melody Y. Guan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zoph%2C+B" target="_blank" rel="noopener">Barret Zoph</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V" target="_blank" rel="noopener">Quoc V. Le</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dean%2C+J" target="_blank" rel="noopener">Jeff Dean</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在ENAS中，控制器通过在大型计算图中搜索最优子图来发现神经网络结构。利用策略梯度对控制器进行训练，选择验证集上期望报酬最大的子图，同时对所选子图对应的模型进行训练，使正则交叉熵损失最小化。</p><p><strong>最关键是利用子模型之间共享参数，大大减少了训练时长。</strong></p><h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p>这项工作的主要贡献是通过强制所有子模型共享权重来提高NAS的效率，从而避免从零开始到收敛对每个子模型进行训练。这个想法有明显的并发症,为不同的子模型可能利用他们的权重不同,但被以前的工作鼓励转移学习和多任务学习,证实为一个特定的模型参数学习的一项特殊的任务可以用于其他模型对其他任务。</p><p>我们的经验表明，不仅在子模型之间共享参数是可能的，而且它还允许非常强大的性能。具体来说，在CIFAR-10上，我们的方法的测试误差为2.89%，而NAS的测试误差为2.65%。在Penn Treebank上，我们的方法实现了一个测试perplexity为55.8，显著优于NAS的测试perplexity为62.4 (Zoph &amp; Le, 2017)，这是Penn Treebank不使用训练后处理的方法中一个新的最先进的方法。重要的是，在我们所有的实验中，我们使用一个Nvidia GTX 1080Ti GPU，搜索架构花费不到16个小时。与NAS相比，这减少了超过1000倍的gpu小时。由于其有效性，我们将我们的方法命名为高效神经结构搜索(ENAS)。</p><h2 id="二-方法"><a href="#二-方法" class="headerlink" title="二 方法"></a>二 方法</h2><p>ENAS的核心思想是观察到NAS最终遍历的所有图都可以看作是更大图的子图。换句话说，我们可以使用一个有向无环图(DAG)来表示NAS的搜索空间。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g34ca8i909j30bk062jrn.jpg" alt></p><p>直观地说，ENAS的DAG是NAS搜索空间中所有可能的子模型的叠加，其中节点表示本地计算，边缘表示信息流。每个节点上的本地计算都有自己的参数，只有在激活特定的计算时才使用这些参数。因此，ENAS的设计允许在搜索空间中的所有子模型(即架构)之间共享参数。</p><h3 id="2-1-RNN中的cell设计"><a href="#2-1-RNN中的cell设计" class="headerlink" title="2.1 RNN中的cell设计"></a>2.1 RNN中的cell设计</h3><p>对于RNN cell的设计，ENAS不同于NAS。NAS将cell架构的拓扑固定为二叉树，只学习树中每个节点的操作。ENAS则被允许在RNN cell中设计拓扑和操作，因此更加灵活。</p><p>ENAS使用了一个包含N个节点的DAG，其中Node表示本地计算，Edge表示N个节点之间的信息流。<strong>ENAS的控制器是一个RNN，它决定:1)哪些边被激活，2)在DAG中的每个节点上执行哪些计算。</strong></p><p>为了创建一个循环单元，控制器RNN对N个决策块进行采样。这里我们通过一个简单的例子说明ENAS机制，N = 4个竞争节点的循环细胞(如图1所示)，设x_t为循环细胞的输入信号(如word embedding)，h_t - 1为前一个时间步的输出。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g34en57kvbj30f505ft96.jpg" alt></p><ol><li><p>在节点1: 控制器首先采样一个激活函数。在本例中，控制器选择tanh激活函数，这意味着RNN的节点1应该计算$h_{1}=\tanh \left(\mathbf{x}_{t} \cdot \mathbf{W}^{(\mathbf{x})}+\mathbf{h}_{t-1} \cdot \mathbf{W}_{1}^{(\mathbf{h})}\right)$</p></li><li><p>在节点2: 控制器然后对之前的索引和激活函数进行采样。在我们的示例中，它选择前面的index 1和激活函数ReLU。Node 2计算$h_{2}=\operatorname{ReLU}\left(h_{1} \cdot \mathbf{W}_{2,1}^{(\mathbf{h})}\right)$</p></li><li><p>在节点3:控制器再次采样之前的索引和激活函数。在我们的示例中，它选择前面的index 2和激活函数ReLU。Node3 可以表示从$h_{3}=\operatorname{ReLU}\left(h_{2} \cdot \mathbf{W}_{3,2}^{(\mathbf{h})}\right)$</p></li><li><p>在节点4:控制器再次采样之前的索引和激活函数。在我们的例子中，它选择前面的index 1和激活函数tanh，得到$h_{4}=\tanh \left(h_{1} \cdot \mathbf{W}_{4,1}^{(\mathbf{h})}\right)$</p></li><li><p>对于输出，我们简单地将所有未被选中的节点平均为所有其他节点的输入。在我们的示例中，由于索引3和索引4从未被采样为任何节点的输入，所以RNN使用它们的平均值(h3 + h4)/2作为输出。也就是说，$h_t = (h_3 + h_4)/2$。</p></li></ol><p>在上面的示例中,对于每一对节点$j &lt;ℓ$,有一个独立的参数matrix$W^{(h)}_{ℓ, j}$。如示例所示，通过选择前面的索引，控制器还可以决定使用哪个参数矩阵。因此，在ENAS中，搜索空间中的所有复发细胞共享相同的一组参数。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g34eqkxei5j30g204v3yt.jpg" alt></p><p>我们的搜索空间包含指数级的构型。具体来说，如果RNN的cell有N个节点，并且我们允许4个激活函数(即tanh、ReLU、identity和sigmoid)，那么搜索空间就是4^N×N!配置。在我们的实验中，N = 12，这意味着在我们的搜索空间中大约有10^15个模型。</p><h3 id="2-2-培训ENAS并派生体系结构"><a href="#2-2-培训ENAS并派生体系结构" class="headerlink" title="2.2 培训ENAS并派生体系结构"></a>2.2 培训ENAS并派生体系结构</h3><p>ENAS控制器网络是一个拥有100个隐藏单元的LSTM (Hochreiter &amp; Schmidhuber, 1997)。这个LSTM通过softmax分类器以自回归的方式对决策进行采样:将上一步中的决策作为嵌入到下一步的输入提供。在第一步，控制器网络接收一个空嵌入作为输入。</p><p>在ENAS,有两套可学的参数:控制器LSTM的参数θ,和子模型的共享参数ω。ENAS的培训过程包括两个交叉阶段。</p><ul><li><p>第一阶段通过一整个训练集训练子模型共享参数w。在本文的Penn Treebank实验中，ω是训练大约400 steps,每个minibatch使用64个样本,采用梯度反向传播+梯度截断（<strong>Truncated Gradient</strong>）进行优化，每隔35 steps进行一次隔断。同时,在CIFAR-10上,ω是45,000张训练图像,分为minibatches大小128,∇ω计算使用标准的反向传播。</p></li><li><p>第二阶段训练参数控制器LSTM的参数θ, 对于固定数量的步骤,通常在我们的实验设置为2000。这两个阶段在ENAS的培训期间交替进行。</p></li></ul><p><strong>Training the shared parameters ω of the child models.</strong></p><p>在这一步中，固定控制器的策略。对w采用SGD获得期望最小值，loss采用标准交叉熵。梯度的计算采用蒙特卡罗估计</p><script type="math/tex; mode=display">\nabla_{\omega} \mathbb{E}_{\mathbf{m} \sim \pi(\mathbf{m} ; \theta)}[\mathcal{L}(\mathbf{m} ; \omega)] \approx \frac{1}{M} \sum_{i=1}^{M} \nabla_{\omega} \mathcal{L}\left(\mathbf{m}_{i}, \omega\right)</script><p>本公式提供了关于梯度的无偏估计，但这个估计比固定m的标准SGD梯度有更高的方差。但令人惊奇的是，我们发现M=1时表现良好，也就是说，我们可以从π(M;θ)中取样任何单一模型Mω，直接使用它的参数w。</p><p><strong>Training the controller parameters θ.</strong></p><p>在这一步中,我们固定ω和更新政策参数θ,旨在最大化期望的奖励Em∼π(m;θ)[R (mω)]。我们使用Adam优化器(Kingma &amp; Ba, 2015)，使用REINFORCE计算梯度，并使用moving average baseline来减少方差。</p><p>奖励R (m，ω)在验证集进行计算,而不是在训练集上,来鼓励ENAS选择泛化更好的模型而不是overfit模型。在我们的语言模型实验中，奖励函数为c/valid ppl，其中的perplexity是在一个验证集的minibatch上计算出来的。在我们的图像分类实验中，奖励函数是验证集上一个minibatch的准确性。</p><p><strong>Deriving Architectures.</strong> </p><p>如何在训练好的ENAS模型中派生出全新的结构。首先从训练好的策略π(m, θ)中采样几个模型。针对每个模型，通过验证集的一个minibatch计算reward。选取其中reward最高的模型从零开始预训练。</p><p>通过一些其他工作可以证明我们这样做是有效而经济的。</p><h3 id="2-3-设计CNN网络"><a href="#2-3-设计CNN网络" class="headerlink" title="2.3 设计CNN网络"></a>2.3 设计CNN网络</h3><p>在卷积模型的搜索空间中，控制器RNN在每个desicion block上采样了两组决策:1)连接到之前的哪个节点，2)使用什么计算操作。这些决策在卷积模型中构建了一个层。类似NAS，ENAS依旧采用anchor来实现skip connection。</p><p>下图中提供了一个采样卷积网络的示例。在本例中，在第k = 4层，控制器对之前的索引{1,3}进行采样，因此第1层和第3层的输出沿着深度维度进行连接并发送到第4层。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g34gb378icj30fr0aemyh.jpg" alt></p><p>同时，使用何种计算操作的决定，将特定的层设置为卷积层、平均池层或最大pooing层。控制器可用的6个操作是:3×3 conv， 5×5 conv，3×3 dw conv， 5×5 dw conv，3×3 maxpooling, 和3×3 average pooling。对于递归cell，ENAS卷积网络中每一层的每个操作都有一组不同的参数。</p><p>在总共进行L次所描述的决策集时，我们可以对L层网络进行采样。由于所有的决策都是独立的，所以搜索空间中存在$6^{L} \times 2^{L(L-1) / 2}$个网络。在我们的实验中，L = 12，得到$1.6×10^{29}$个可能的网络。</p><h3 id="2-4-设计卷积cell"><a href="#2-4-设计卷积cell" class="headerlink" title="2.4 设计卷积cell"></a>2.4 设计卷积cell</h3><p>同样的，可以只设计简单的卷积模块-cell，再进行堆叠。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g34gi5nw3bj30g103xdg0.jpg" alt></p><p>我们使用带有B节点的ENAS的可计算DAG来表示每个cell中的计算过程。在DAG中,节点1和节点2被视为细胞的输入，它们其实为前两个cell的最后输出。对于每个剩余的B−2个节点, 控制器RNN需要做两套决定:1)哪两个节点作为当前节点的输入，2)用于这两个采样节点的两个操作。可用的5种操作有:identity, 3×3 separable convolution,  5×5 separable convolution, 3×3 average pooling,  3×3 max pooling。在每个节点上，采样前一个节点及其对应的操作之后，将这些操作应用于前一个节点，并添加它们的结果。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g34girofugj30ga053jry.jpg" alt></p><p>和前面一样，我们用一个例子来说明搜索空间的机制，这里是B = 4个节点(参见图5)。</p><ol><li><p>节点1,2是输入节点，因此不需要为它们做任何决策。设$h_1$,$ h_2$为这些节点的输出。</p></li><li><p>在节点3:控制器采样前两个节点和两个操作。在左上角的图5中，它对节点1、节点2分别选取了操作sep conv 5x5和identity。这意味着$h_3 = {sep_conv_5x5}(h_2) + id(h_2)$。</p></li><li><p>在节点4:控制器采样节点3、节点1为 avg pooling3x3和sep conv 3x3。这意味着$h_4 = avg_pool_3x3(h_3) + sep_conv_3x3(h_1)$。</p></li><li><p>由于除h4之外的所有节点都被用作至少另一个节点的输入，因此惟一的松散端h4被视为单元的输出。如果有多个松散的端点，它们将沿着深度维度连接起来，形成单元格的输出。</p></li></ol><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g36o1yolgqj308f0bg3yx.jpg" alt></p><p>在我们讨论的搜索空间中也可以实现一个reduction cell，方法很简单:</p><ol><li><p>从搜索空间中抽取一个计算图</p></li><li><p>以2为步长应用所有operations</p></li></ol><p>因此，约简单元将其输入的空间维度减少了2倍。跟随Zoph等人(2018)，我们对卷积细胞上的reduction cell进行采样，从而使控制RNN总共运行2个(B - 2)块。</p><p>最后，我们估计了这个搜索空间的复杂度。在节点$i(3 \leq i \leq B)$处，控制器可以从$i−1$个节点中任意选择2个节点，从5个操作中任意选择2个操作。由于所有的决策都是独立的，所以有$(5×(B - 2)!)^2$个可能的单元。由于我们分别对normal cell和reduction cell进行采样，因此搜索空间的最终大小为$(5×(B - 2)!)^4$。在我们的实验中，当$B = 7$时，搜索空间可以实现$1.3×10^{11}$个最终网络，使得搜索空间明显小于整个卷积网络的搜索空间(第2.3节)。</p><h2 id="三-实验结果"><a href="#三-实验结果" class="headerlink" title="三  实验结果"></a>三  实验结果</h2><p>主实验为RNN实验：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g36mbgiy7yj30sc0b4n01.jpg" alt></p><p>在CNN上的表现：<strong>最重要的，通过权重共享，现在ENAS的训练时长是一块1080TI训练0.45天</strong></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g34gt7rwbhj30yn0kwdmf.jpg" alt></p><p>训练得到的cell如下：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g36mama6vej30dy0mndhq.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      在ENAS中，控制器通过在大型计算图中搜索最优子图来发现神经网络结构。利用策略梯度对控制器进行训练，选择验证集上期望报酬最大的子图，同时对所选子图对应的模型进行训练，使正则交叉熵损失最小化。最关键是利用子模型之间共享参数，大大减少了训练时长。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="Google Brain" scheme="https://jiaoyuzhang.github.io/tags/Google-Brain/"/>
    
      <category term="ENAS" scheme="https://jiaoyuzhang.github.io/tags/ENAS/"/>
    
      <category term="weight sharing" scheme="https://jiaoyuzhang.github.io/tags/weight-sharing/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-NASNet</title>
    <link href="https://jiaoyuzhang.github.io/2019/05/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-NASNet/"/>
    <id>https://jiaoyuzhang.github.io/2019/05/12/论文阅读-NASNet/</id>
    <published>2019-05-12T05:37:20.000Z</published>
    <updated>2019-05-17T06:49:54.200Z</updated>
    
    <content type="html"><![CDATA[<p>论文： Learning Transferable Architectures for Scalable Image Recognition 【<a href="https://arxiv.org/abs/1707.07012" target="_blank" rel="noopener">pdf</a>】</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zoph%2C+B" target="_blank" rel="noopener">Barret Zoph</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vasudevan%2C+V" target="_blank" rel="noopener">Vijay Vasudevan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shlens%2C+J" target="_blank" rel="noopener">Jonathon Shlens</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V" target="_blank" rel="noopener">Quoc V. Le</a></p><p>CVPR 2018</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使用强化学习寻找最优网络，包括CNN：一个图像分类网络的卷积部分（表示层）、RNN：一个类似于LSTM的cell。</p><p>相较于NAS的优化：</p><p>一种新的搜索空间- NASNet search space—-a generic convolutional cell</p><p>一种新的正则化技术，ScheduledDropPath，使得NASNet模型的泛化能力更好</p><h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>NASNet会使用一个RNN构成的控制器（controller）以概率p随机（policy gradient）采样一个网络结构A，接着在CIFAR-10上训练这个网络并得到其在验证集上的精度R，然后在使用R更新控制器的参数，如此循环执行直到模型收敛，如图1所示。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g33jv2jjvjj30c905twer.jpg" alt></p><h2 id="新的搜索空间"><a href="#新的搜索空间" class="headerlink" title="新的搜索空间"></a>新的搜索空间</h2><p>不再进行整个网络的搜索，而是定义了cell单元作为building block。通过堆叠cell形成最后的网络。</p><p>具体讨论的话，对于CNN网络，采用了两种cell，<strong>normal cell</strong>和<strong>reduction cell</strong>。normal cell的输出保持维度不变，而reduction cell用于收缩维度，具体来说就是步长为2。两种单元是不同的架构，堆叠在一起形成了最后的网络架构。</p><p>控制器RNN输出的是两种cell的超参，最后以下图形式堆叠为子网络，进行训练和验证，得到验证集准确率R回馈到控制器。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1g33kbk8n78j30a60c8q3b.jpg" alt></p><h2 id="cell的搜索方式"><a href="#cell的搜索方式" class="headerlink" title="cell的搜索方式"></a>cell的搜索方式</h2><p>在NASNet的搜索空间中，每个cell的有两个初始隐藏状态输入$h_i$和$h_{i-1}$，分别是前两个层的输出。控制器RNN会跟根据这两个initial hidden states预测卷积cell的剩余结构。</p><p>控制器对每个cell的预测分为B个bolck（此处的block可以理解为一个操作），每个block又有5个不同的softmax分类器进行5个预测步骤。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1g33kj63rqjj307a05zwek.jpg" alt></p><p>Step 1. Select a hidden state from hi, hi−1 or from the set of hidden states created in previous blocks.</p><p>Step 2. Select a second hidden state from the same options as in Step 1. </p><p>Step 3. Select an operation to apply to the hidden state selected in Step 1. </p><p>Step 4. Select an operation to apply to the hidden state selected in Step 2.</p><p>Step 5. Select a method to combine the outputs of Step 3 and 4 to create a new hidden state.</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g33kkdncikj30ki05yjs2.jpg" alt></p><p>在step3和step4中，选择的操作有一下这几种：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g33klrpeuhj30d4047dgc.jpg" alt></p><p>在step5中，选择像素级相加或者直接拼接，最后所有conv cell中产生的未使用hidden states 在depth维度被拼接在一起作为最后输出。</p><p>在算法中，用之前已经存在的隐藏态作为序列分块的可能输入来填入新建的hidden state。B=5时效果较好，即一个cell中包含5个操作。</p><p>为了使RNN能同时预测Normal Cell和Reduction Cell，我们简单设定控制器有2*5B个预测步骤。前5B给Normal Cell，后5B给Reduction Cell。</p><h2 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h2><h3 id="搜索策略的选择"><a href="#搜索策略的选择" class="headerlink" title="搜索策略的选择"></a>搜索策略的选择</h3><p>NASNet采用了NAS中的强化学习方法，同时也尝试了随机搜索的策略。</p><p>在随机搜索中，NASNet从均匀分布中对决策进行抽样，而不是从控制器RNN中的softmax分类器中对决策进行抽样。</p><p>在实验中，我们发现随机搜索比CIFAR-10数据集上的强化学习略差。虽然使用强化学习是有价值的，但与文献[71]中发现的差距要小。这一结果表明，1)NASNet搜索空间构造良好，随机搜索可以执行得相当好，2)随机搜索是一个很难超越的基线。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g349ju6316j30du0dd760.jpg" alt></p><h3 id="ScheduledDropPath"><a href="#ScheduledDropPath" class="headerlink" title="ScheduledDropPath"></a>ScheduledDropPath</h3><p>具体实验中，NASNet试用几种不同的随机正则化方法，发现简单的在过滤器上采用dropout会较低效果。所以采用了一种DropPath的改进方法ScheduledDropPath。具体来说，就是在DropPath中，NASNet随机地以一些固定的概率丢弃单元格中的每条路径（即下图中黄色框的边）</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g349twrvf4j30m90ckmyy.jpg" alt></p><p>有趣的是，我们还发现单独使用DropPath对NASNet训练没有多大帮助，但在训练中线性增加dropout概率的DropPath，显着提高了CIFAR和ImageNet实验的最终性能。我们将此方法命名为ScheduledDropPath</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在具体训练阶段，控制器采用Proximal Policy Optimazation（PPO）策略，用RNN控制一个全局工作序列系统来生成一个子网络的备选池。</p><p>训练时长为500个GPU训练4天，总计2000GPU时。比初始NAS快了7倍，但是还是非常奢侈。</p><p>训练得到的卷积cell：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g349twrvf4j30m90ckmyy.jpg" alt></p><p>CIFAR-10上的训练效果：</p><p><img src="/Users/vera/Library/Application Support/typora-user-images/image-20190517140203982.png" alt="image-20190517140203982"></p><p>cell泛化到ImageNet的图像分类任务：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g34a1lfussj30p609b0ux.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      NASNet，Google的强化学习神经网络架构搜索，沿用了前作NAS的基本思想，并重新定义了search space。以cell为搜索空间，通过堆叠cell架构网络。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="NASNet" scheme="https://jiaoyuzhang.github.io/tags/NASNet/"/>
    
      <category term="Google Brain" scheme="https://jiaoyuzhang.github.io/tags/Google-Brain/"/>
    
  </entry>
  
  <entry>
    <title>综述-nerual architecture search</title>
    <link href="https://jiaoyuzhang.github.io/2019/05/12/%E7%BB%BC%E8%BF%B0-nerual-architecture-search/"/>
    <id>https://jiaoyuzhang.github.io/2019/05/12/综述-nerual-architecture-search/</id>
    <published>2019-05-12T05:36:59.000Z</published>
    <updated>2019-05-21T08:17:38.060Z</updated>
    
    <content type="html"><![CDATA[<p>综述-Nerual Architecture Search </p><p>神经网络架构搜索，顾名思义，就是让机器自己去学习如何构架一个神经网络，得到比人类专家手工设计的网络更好的效果。这个思路是非常一脉相承的，就像机器学习到深度学习，也是完成了人类专家手工提取特征到由机器自己学习特征这样的步骤转换。</p><p>而现在的深度学习网络架构，虽然获得了不错的效果，但是细究起来，其实是没有非常牢固的理论根据的，依靠的是人类的先验与设计。</p><p>所以，在考虑有限制的空间内获得效果更好的网络这一问题上，架构搜索能够给出一些答案。</p><h2 id="Introduction-of-NAS"><a href="#Introduction-of-NAS" class="headerlink" title="Introduction of NAS"></a>Introduction of NAS</h2><p>主参考论文：Neural Architecture Search: A Survey   【<a href="https://arxiv.org/abs/1808.05377" target="_blank" rel="noopener">pdf</a>】</p><p>作者：<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Elsken%2C+T" target="_blank" rel="noopener">Thomas Elsken</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Metzen%2C+J+H" target="_blank" rel="noopener">Jan Hendrik Metzen</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Hutter%2C+F" target="_blank" rel="noopener">Frank Hutter</a></p><p>首先，我们先整体讨论一下神经网络架构搜索到底是怎样进行的。通常来说，我们需要考虑三个方面：搜索空间、搜索策略和评价策略。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190520231434.png" alt></p><p><strong>搜索空间（SearchSpace）</strong>:搜索空间定义了搜索的范围，其实就是在哪搜索。通过结合一些过去研究者架构设计方面的经验，可以通过减小搜索空间和简化搜索过程来提高搜索的性能。当然，这样同时也引入了人为的主观臆断，可能会妨碍寻找到超越当前人类知识的新的架构构建块（buildingblocks）。</p><p><strong>搜索策略（Searchstrategy）</strong>：搜索策略定义的则怎样去搜索。一方面，我们希望能快速找到性能良好的架构，另一方面，也应避免过早收敛到次优架构（suboptimalarchiteture）区域。</p><p><strong>性能评估策略（Performaceestimation strategy）</strong>：NAS 的目标是希望能够自动的在给定的数据集上找到一个高性能的架构。性能评估则是指评估此性能的过程：最简单的方式是按照通常的方式对一个标准架构训练和验证来获得结果，但遗憾的是这样的计算成本太高了，并且同时限制了可以搜索的网络架构的数量。因此，最近的许多研究都集中在探索新的方法来降低这些性能评估的成本。</p><h3 id="搜索空间（Search-Space）"><a href="#搜索空间（Search-Space）" class="headerlink" title="搜索空间（Search Space）"></a>搜索空间（Search Space）</h3><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190520231408.png" alt></p><p>如图所示的就是一个相对简单的搜索空间，称为<strong>链式神经网络（Chain-stuctured Neural Network）</strong>，很简单就是一个链表，第 $i-1$层的输出作为第 $i$ 层的输入，可以表示为 $A=L_{n} \circ \ldots L_{1} \circ L_{0}$。而针对于这样一个 search space，我们就需要考虑这些参数：</p><ul><li>网络的(最大)层数$n$</li><li>每一层执行的操作类型，比如 pooling， convolution 这样的基础操作，或者更高级的一些操作，如depthwise separable convolutions 或 dilated convolutions。</li><li>每一层与这个操作相关的 hyperparameters，比如对于一个一般的 convolutional 层来说，有 filter 的 numbers，keneral 的 size 和 strides 的 length，而对于 fully-connected 层来说就说是 units 的 number 了。</li></ul><p>而需要注意的是，与每层相关的 hyperparameters 是取决于这一层的操作类型的，因此对于 Search Space 的参数化的结果并不是一个固定的长度（fixed-length），而是一个条件空间（conditioanl space）。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190520231742.png" alt></p><p>最近的很多关于 NAS 的研究中都引入了人工设计出的如跳跃连接(skip connections)这样的架构元素，可以用来构建如图 2 右所示的复杂的多分支网络（multi-branch networks）。对于这样的结构，第 i 层的输入不再仅仅是前一层的输入，而需要表示为一个组合函数的形式。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190520231653.png" alt></p><p>这种 cell-based 的 search space 也应用在了很多后来的研究中。然而，当使用基于 cell 的搜索空间时，出现了一种新的选择问题，即如何选择元架构（Micro-architecture）：应该使用多少 cells 以及如何连接它们来构建实际模型？</p><p>理想情况下，Meta-architecture 也应作为 NAS 的一部分自动优化;否则，如果大多数复杂性已经由 meta-architecture 解决，那么很容易就把问题变成进行 meta-architecture engineer，这样对 Cell 的搜索就变得过于简单了。</p><h3 id="搜索策略（Search-Strategy）"><a href="#搜索策略（Search-Strategy）" class="headerlink" title="搜索策略（Search Strategy）"></a>搜索策略（Search Strategy）</h3><p>到现在，已经有许多不同的搜索策略用于 NAS，主要有如下这些：</p><ul><li>随机搜索（random search）</li><li>贝叶斯优化（Bayesian optimazation）</li><li>进化方法（evolutionaray methods）</li><li>强化学习（Reinforcement Learning, RL）</li><li>梯度方法（gradient-based methods）</li></ul><p>自 2013 年开始，Bayesian optimazation（BO）就在 NAS 研究中取得了一些成功，基于 BO 发现了当时最优的 vison architectures，在 CIFAR-10 上取得最优 architeture，以及实现了第一个超过人类专家赢得竞赛的 automaticallytuned 神经网络。</p><p>在 16 年 Google Brain 发表的 [10] 中通过强化学习的搜索策略在 CIFAR-10 和 Penn Treebank 数据集上取得很好表现后，NAS 成为机器学习社区的主流研究课题之一。</p><p>关于其他几个算法，会在后续介绍，这边简单介绍一下进化算法和贝叶斯优化。</p><p><strong>贝叶斯优化（BO）</strong>是用于 hyperparameters 优化中最流行的方法，但是由于 typical BO toolboxes 是基于高斯过程且主要集中于低维连续性优化问题，所以它并没有被很多组应用于 NAS 中。[17] 中派生了 kennel function 来使用基于 GP 的 BO 方法，但还没有实现过最优的结果。相比之下，一些研究中使用基于树的模型或随机森林来搜索非常高维的条件空间时实现了在很多问题上的最优性能，这些方法同时优化神经架构和它们的 hyperparameters。虽然缺乏完整的比较，但初步证据表明这些方法可以超过进化算法 [18]。</p><p>关于 进化算法：</p><p>更近期的一些研究中有使用基于梯度的方法来优化权重，而使用进化算法来优化神经结构本身。是用进化算法演化一组模型，在每个进化步骤中，对来自群体的至少一个模型进行采样，并将它们作为父母通过繁衍或者突变来产生后代。在 NAS 中，突变表示本地化的操作，例如添加或移除层，改变层的超参数，添加跳跃连接，以及改变训练超参数。在对后代进行训练之后，评估它们的适应性（例如，在验证集上的表现）后再将它们添加到种群中。</p><h3 id="性能评估策略（Performace-Estimation-Strategy）"><a href="#性能评估策略（Performace-Estimation-Strategy）" class="headerlink" title="性能评估策略（Performace Estimation Strategy）"></a>性能评估策略（Performace Estimation Strategy）</h3><p>上一节讨论的搜索策略旨在找到某些性能度量（如准确度）最大化的架构 A，为了引导它们的搜索过程，这些策略需要考虑如何评判给定架构的性能高低。最简单的方法是在训练数据上训练 A 并评估其在验证数据上的表现。然而，从头开始训练每个要评估的架构经常会产生大约数千 GPU 天的计算需求</p><p>为了加快搜索过程，需要在相对较少的评估的基础上并在相对较大的搜索空间中进行良好的预测</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190520231911.png" alt></p><ul><li>Lower Fidelit Estimates</li></ul><p>这种 lower fidelities 可以表示为较短的训练时间，对训练子集的训练，使用较低分辨率的图像，或者是用更少的 filter。虽然这些 lower fidelities 降低了计算成本，但与此同时它们也会在测试中引入偏差，因为性能通常会被低估。当然只要搜索策略只依赖于相对排名时，这不会有什么问题。</p><ul><li>Learning Curve Extrapolation</li></ul><p>提出方法来推断初始学习曲线并终止那些预测表现不佳以加速架构搜索过程。另外一些研究者则通过基于架构的超参数来预测哪些部分学习曲线最有希望。</p><ul><li>Weight Inheritance/ Network Morphisms</li></ul><p>基于之前已经训练过的其他架构的权重来初始化新架构的权重。实现这一目标的一种方法，称为 network morphisms，它允许修改架构的同时保持网络所代表的功能不变。这就允许连续的增加网络容量并保持高性能而无需从头开始训练。对几个时期的持续训练也可以利用 network morphisms 引入额外容量。</p><ul><li>One-Shot Models/ Weight Sharing</li></ul><p>它将所有架构都视为一个超级图的不同子图，并在架构之间共享权重。那么只需要一次性训练单个模型的权重，然后通过继承权重来评估架构（它们只是一次性模型的子图），而无需再进行任何单独的训练。比如ENAS、Darts。</p><h2 id="NAS-amp-NASNet-amp-ENAS"><a href="#NAS-amp-NASNet-amp-ENAS" class="headerlink" title="NAS &amp; NASNet &amp; ENAS"></a>NAS &amp; NASNet &amp; ENAS</h2><h3 id="NAS"><a href="#NAS" class="headerlink" title="NAS"></a>NAS</h3><p>Neural Architecture Search with Reinforcement Learning 【<a href="https://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">pdf</a>】</p><p>它是NSANet的前置，比较详细的介绍了Google的NAS的整体设计，包括具体的目标函数以及实现skip connection的具体方法，主要是提出了用强化学习的方法来完成搜索神经网络架构。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521143954.png" alt></p><p>通过一个controllerRNN在搜索空间（search space）中得到一个网络结构（论文中称为child network），然后用这个网络结构在数据集上训练，在验证集上测试得到准确率R，再将这个准确率回传给controller，controller继续优化得到另一个网络结构，如此反复进行直到得到最佳的结果，整个过程称为Neural Architecture Search。</p><h4 id="How-to-use-a-controller-RNN-to-generate-an-CNN-model"><a href="#How-to-use-a-controller-RNN-to-generate-an-CNN-model" class="headerlink" title="How to use a controller RNN to generate an CNN model"></a><strong>How to use a controller RNN to generate an CNN model</strong></h4><p>控制器生成的是网络架构中的超参数，NAS中假定预测的inference中只含卷积层。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144036.png" alt></p><p>对于Layer N，控制器可以预测该层的filter width，filter height，stride height，stride width，filter的个数，每一个预测都是通过softmax分类，并作为下一个预测的输入。实验中的停止条件是，层的个数超过了某个指定的数值。当然这个数值在训练的过程中也可以变化。</p><p>最终得到的是LSTM产生的一个序列化的tokens，记为$\theta_{\mathrm{c}}$。</p><h4 id="How-to-train-a-controller-RNN-with-REINFORCE"><a href="#How-to-train-a-controller-RNN-with-REINFORCE" class="headerlink" title="How to train a controller RNN with REINFORCE"></a><strong>How to train a controller RNN with REINFORCE</strong></h4><p>将NAS与强化学习做一个对应，那么控制器LSTM就是Agent，控制器预测产生$\theta_{\mathrm{c}}$对应为活动$a_{1 : T}$，子网络在验证集上的准确率R作为reward signal。</p><p>那么，就得到了目标函数：</p><script type="math/tex; mode=display">J\left(\theta_{c}\right)=E_{P\left(a_{1 : T} ; \theta_{c}\right)}[R]</script><p>鉴于R是不连续的，我们采用policy gradient来迭代更新$\theta_{\mathrm{c}}$，具体采用了REINFORCE规则：</p><script type="math/tex; mode=display">\nabla_{\theta_{c}} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1 : T} ; \theta_{c}\right)}\left[\nabla_{\theta_{c}} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R\right]</script><p>对上述公式作一个先验近似：</p><script type="math/tex; mode=display">\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla \theta_{c} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R_{k}</script><p>上述公式中m为controller中一个batch的大小，也就是一个batch中的不同架构数。T表示一个网络架构中超参个数。$R_{k}$是第k个网络架构的准确率。</p><p>上述是梯度的一种无偏估计，但是方差很高。为减小方差，添加了一个baseline：</p><script type="math/tex; mode=display">\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_{c}} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right)\left(R_{k}-b\right)</script><p>由于baseline fuction b不依赖与当下action，本式依然是一个无偏估计。b采用的之前架构准确率的指数加权平均（exponential moving average）。</p><h4 id="How-to-add-skip-connections-and-other-layer-types"><a href="#How-to-add-skip-connections-and-other-layer-types" class="headerlink" title="How to add skip connections and other layer types"></a><strong>How to add skip connections and other layer types</strong></h4><p>NAS采用注意力机制。具体来说，通过添加在第N层添加N-1个anchor，来确定是否要与之前的某一层跳跃连接。anchor通过两层（本层和上一层）的hidden state，用sigmoid来判断：</p><script type="math/tex; mode=display">\mathrm{P}(\text { Layer } \mathrm{j} \text { is an input to layer i })=\operatorname{sigmoid}\left(v^{\mathrm{T}} \tanh \left(W_{\text {prev}} * h_{j}+W_{\text {curr}} * h_{i}\right)\right)</script><p>公式中的$W_prev$、$W_curr$和$v$是可训练变量。这些行为依然是概率分布的，因此REINFORCE方法不会有大的改动。</p><p>由下图可以形象的了解anchor point是怎样添加跳跃连接的。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144116.png" alt></p><p>本篇的实验结果并无太多参考意义，因为很快作者就出了NASNet。NAS最大的意义还是在提出了一种神经网络搜索的框架结构。</p><p>实验中采用了2层35个隐藏单元的LSTM，使用800个GPU训练了28天，一共22400 GPU-Hours，一共训练了12800个架构。</p><h3 id="NASNet"><a href="#NASNet" class="headerlink" title="NASNet"></a>NASNet</h3><p>论文： Learning Transferable Architectures for Scalable Image Recognition 【<a href="https://arxiv.org/abs/1707.07012" target="_blank" rel="noopener">pdf</a>】</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144151.png" alt></p><p>相较于NAS的优化：</p><p>一种新的搜索空间- NASNet search space—-a generic convolutional cell</p><p>一种新的正则化技术，ScheduledDropPath，使得NASNet模型的泛化能力更好(实验细节，不太重要)</p><h4 id="NASNet-cell"><a href="#NASNet-cell" class="headerlink" title="NASNet cell"></a><strong>NASNet cell</strong></h4><p>不再进行整个网络的搜索，而是定义了cell单元作为building block。通过堆叠cell形成最后的网络。</p><p>具体讨论的话，对于CNN网络，采用了两种cell，<strong>normal cell</strong>和<strong>reduction cell</strong>。normal cell的输出保持维度不变，而reduction cell用于收缩维度，具体来说就是步长为2。两种单元是不同的架构，堆叠在一起形成了最后的网络架构。</p><p>控制器RNN输出的是两种cell的超参，最后以下图形式堆叠为子网络，进行训练和验证，得到验证集准确率R回馈到控制器。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144401.png" alt></p><p>在NASNet的搜索空间中，每个cell的有两个初始隐藏状态输入$h_i$和$h_{i-1}$，分别是前两个层的输出。控制器RNN会跟根据这两个initial hidden states预测卷积cell的剩余结构。</p><p>控制器对每个cell的预测分为B个bolck（此处的block可以理解为一个操作），每个block又有5个不同的softmax分类器进行5个预测步骤。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144305.png" alt></p><p>在算法中，用之前已经存在的隐藏态作为序列分块的可能输入来填入新建的hidden state。B=5时效果较好，即一个cell中包含5个操作。</p><p>为了使RNN能同时预测Normal Cell和Reduction Cell，我们简单设定控制器有2*5B个预测步骤。前5B给Normal Cell，后5B给Reduction Cell。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h4><p>在具体训练阶段，控制器采用Proximal Policy Optimazation（PPO）策略，用RNN控制一个全局工作序列系统来生成一个子网络的备选池。</p><p>训练时长为500个GPU训练4天，总计2000GPU时。比初始NAS快了7倍，但是还是非常奢侈。</p><p>训练得到的卷积cell：</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144438.png" alt></p><p>CIFAR-10上的训练效果：</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144623.png" alt></p><p>cell泛化到ImageNet的图像分类任务：</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521144704.png" alt></p><h3 id="ENAS"><a href="#ENAS" class="headerlink" title="ENAS"></a>ENAS</h3><p>论文：Efficient Neural Architecture Search via Parameter Sharing  【<a href="https://arxiv.org/abs/1802.03268" target="_blank" rel="noopener">pdf</a>】【<a href="https://github.com/melodyguan/enas" target="_blank" rel="noopener">code-tf</a>】【<a href="https://github.com/carpedm20/ENAS-pytorch" target="_blank" rel="noopener">code-python</a>】</p><p>在ENAS中，控制器通过在大型计算图中搜索最优子图来发现神经网络结构。利用策略梯度对控制器进行训练，通过权重共享，快速选择验证集上期望报酬最大的子图。</p><p><strong>最关键是利用子模型之间共享参数，大大减少了训练时长。</strong>采用了控制器参数与子网络参数相互迭代的方式进行训练。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521145805.png" alt></p><p>本处为了通篇逻辑，也使用ENAS中CNN cell的训练过程来描述。RNN网络训练过程请参考原文。</p><h4 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a><strong>搜索空间</strong></h4><p>基本沿用NASNet。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521152634.png" alt></p><p>用一个例子来说明搜索空间的机制，这里是B = 4个节点(参见图5)。</p><ol><li>节点1,2是输入节点，因此不需要为它们做任何决策。设$h_1$,$ h_2$为这些节点的输出。</li><li>在节点3:控制器采样前两个节点和两个操作。在左上角的图5中，它对节点1、节点2分别选取了操作sep conv 5x5和identity。这意味着$h_3 = {sep_conv_5x5}(h_2) + id(h_2)$。</li><li>在节点4:控制器采样节点3、节点1为 avg pooling3x3和sep conv 3x3。这意味着$h_4 = avg_pool_3x3(h_3) + sep_conv_3x3(h_1)$。</li><li>由于除h4之外的所有节点都被用作至少另一个节点的输入，因此惟一的松散端h4被视为单元的输出。如果有多个松散的端点，它们将沿着深度维度连接起来，形成单元格的输出。</li></ol><h4 id="架构搜索与训练权重迭代进行"><a href="#架构搜索与训练权重迭代进行" class="headerlink" title="架构搜索与训练权重迭代进行"></a><strong>架构搜索与训练权重迭代进行</strong></h4><p>在ENAS,有两套可学的参数:控制器LSTM的参数θ,和子模型的共享参数ω。ENAS的培训过程包括两个交叉阶段。</p><ul><li>第一阶段通过一整个训练集训练子模型共享参数w。在本文的Penn Treebank实验中，ω是训练大约400 steps,每个minibatch使用64个样本,采用梯度反向传播+梯度截断（<strong>Truncated Gradient</strong>）进行优化，每隔35 steps进行一次隔断。同时,在CIFAR-10上,ω是45,000张训练图像,分为minibatches大小128,∇ω计算使用标准的反向传播。</li><li>第二阶段训练参数控制器LSTM的参数θ, 对于固定数量的步骤,通常在我们的实验设置为2000。这两个阶段在ENAS的培训期间交替进行。</li></ul><p><strong>Deriving Architectures.</strong> </p><p>如何在训练好的ENAS模型中派生出全新的结构。首先从训练好的策略π(m, θ)中采样几个模型。针对每个模型，通过验证集的一个minibatch计算reward。选取其中reward最高的模型从零开始预训练。</p><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h4><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521152749.png" alt></p><h2 id="Latest-architecture-search-algorithms"><a href="#Latest-architecture-search-algorithms" class="headerlink" title="Latest architecture search algorithms"></a>Latest architecture search algorithms</h2><p>上述NAS三篇论文是一脉相承，一点点改进过来的。其实我最开始接触的论文是Darts，当时感觉这个结构是全新的。再一路看下来，会发现Darts也是在ENAS上又做了搜索策略部分的革新，整体的架构也还是继承下来的。</p><h3 id="Darts"><a href="#Darts" class="headerlink" title="Darts"></a>Darts</h3><p>论文： DARTS: Differentiable Architecture Search 【<a href="https://arxiv.org/abs/1806.09055" target="_blank" rel="noopener">pdf</a>】【 <a href="https://github.com/quark0/darts" target="_blank" rel="noopener">code</a>】</p><p>本文中最大的改进就是把选择op这样一个不可微操作通过softmax放宽为可微函数，由此使得在架构选择时可以采用梯度下降的方法来解决。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521152833.png" alt></p><p>darts的整体流程：<br><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521153027.png" alt></p><ol><li>初始化时，每条边即操作是未知的</li><li>采用松弛策略，使每条边上都进行多种候选操作</li><li>采用bilevel optimazation来联合优化操作的混合概率和网络权重</li><li>采用混合操作中概率最高的操作来替换混合操作，得到最终结构</li></ol><h4 id="搜索空间的可微化处理"><a href="#搜索空间的可微化处理" class="headerlink" title="搜索空间的可微化处理"></a><strong>搜索空间的可微化处理</strong></h4><p>把搜索空间表示为</p><script type="math/tex; mode=display">x^{(i)}=\sum_{j<i} o^{(i, j)}\left(x^{(j)}\right)</script><p>每个node被记为 $x^{(i)}$是一个特征表示（比如feature map）。而每条边edge被记为$o^{(i,j)}$，它表示的是从节$x^{(i)}$到$x^{(j)}$所采用的操作（比如3*3卷积，最大池化），当然也可以是特殊的无操作$zero$。</p><p>darts通过引入softmax来做了一个简单的松弛方案，使得整个搜索空间变为可微的:</p><script type="math/tex; mode=display">\overline{O}^{(i, j)}(x)=\sum_{o \in \mathcal{O}} \frac{\exp \left(\alpha_{o}^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)} o(x)</script><p>其中<script type="math/tex">\alpha^{(i, j)}</script>是一个向量，表示的是节点$ (i, j) $中的混合操作，$\overline{o}^{(i, j)}$是混合概率操作， $ \mathcal{O} $是候选操作集合。</p><p>$\alpha$ 就是整体框架的向量表示。</p><h4 id="迭代训练框架参数-alpha-和权重参数-w"><a href="#迭代训练框架参数-alpha-和权重参数-w" class="headerlink" title="迭代训练框架参数$\alpha$和权重参数$w$"></a><strong>迭代训练框架参数$\alpha$和权重参数$w$</strong></h4><p>目标函数：    </p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{\alpha}} & {\mathcal{L}_{v a l}\left(w^{*}(\alpha), \alpha\right)} \\ {\text { s.t. }} & {w^{*}(\alpha)=\operatorname{argmin}_{w} \mathcal{L}_{\text {train}}(w, \alpha)}\end{array}</script><p>darts中采用了近似迭代的方法来解决这个问题，通过$w$和$\alpha$分别在权重和构架空间中的梯度下降步骤之间交替来完成优化。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521153203.png" alt></p><h4 id="实验结果-2"><a href="#实验结果-2" class="headerlink" title="实验结果"></a><strong>实验结果</strong></h4><p>在CIFAR-10上训练效果：</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521153332.png" alt></p><p>将cell迁移到ImageNet上的结果：</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521153431.png" alt></p><h3 id="RandWiredNN"><a href="#RandWiredNN" class="headerlink" title="RandWiredNN"></a>RandWiredNN</h3><p>论文： Exploring Randomly Wired Neural Networks for Image Recognition 【<a href="https://arxiv.org/abs/1904.01569" target="_blank" rel="noopener">pdf</a>】</p><p>RandWiredNN是思路比较不同的一篇，它把关注点投注在了网络连接上而不是操作选择上，整体思路有了一个突破。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521154704.png" alt></p><p>RAndWireNN 基本思想是研究设计<strong>stochastic network generator</strong>，也就是设计网络构架的机制，它的关注点在网络的连接方式上。</p><p>论文的主要工作包含以下步骤：</p><p>（1）基于图论的随机图方法生成随机图Random Graph；</p><p>（2）将Random Graph转换为一个神经网络NN；</p><p>（3）将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN；</p><p>（4）在ImageNet 1000-class任务上验证RandWireNN的表现；</p><p>（5）重复（1）到（4）步骤。</p><h4 id="一、基于图论的随机图方法生成随机图Random-Graph"><a href="#一、基于图论的随机图方法生成随机图Random-Graph" class="headerlink" title="一、基于图论的随机图方法生成随机图Random Graph"></a><strong>一、基于图论的随机图方法生成随机图Random Graph</strong></h4><p>作者引入了三种随机图生成方法，即Erdos-Renyi（ER）、Barabasi-Albert（BA）和 Watts-Strogatz（WS）。这三个方法生成随机图的机制比较简单。</p><ul><li>ER: N个节点，节点两两之间以P的概率有一条边。该方法包含一个参数P，故以ER(P)表示。</li><li>BA: 初始有M个节点（M&lt;N），每增加1个新节点的时候，该节点以一定的概率与已有的所有节点相连（这个概率与已有节点的度有关），重复，直到这个新节点有M条边。重复，直到整个图有N个节点。该方法包含一个参数M，故以BA(M)表示。</li><li>WS: 所有N个节点排成一个圈，每个节点与两边邻近的K/2个节点相连。之后按照顺时针方向遍历每一个节点，与当前节点相连的某一条边以概率P与其他某个节点相连。重复K/2次。该方法包含2个参数K和P，故以WS(K,P)表示。</li></ul><p>其中模型的节点总数N由论文作者根据网络复杂度（FLOPs）手动指定，因此ER方法的参数搜索空间为P∈[0.0,1.0]，BA方法的参数搜索空间为M∈[1,N]，WS方法的参数搜索空间为K∈[1,N-1] x P∈[0.0,1.0]。图1是三种方法生成的随机图。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155016.png" alt><strong>二、把生成的随机图Random Graph转换为一个神经网络NN</strong></p><h4 id="二、将Random-Graph转换为一个神经网络NN"><a href="#二、将Random-Graph转换为一个神经网络NN" class="headerlink" title="二、将Random Graph转换为一个神经网络NN"></a>二、将Random Graph转换为一个神经网络NN</h4><p><strong>将随机图转化为DAG</strong></p><p>首先要给每条边指定一个方向，即把生成的随机图Random Graph转换为有向无环图DAG。方法就是给每个节点分配一个索引index（从1到N），若两个节点之间有边，则边的方向从小索引节点到大索引节点。其中ER方法按照随机的方式给N个节点分配索引；BA方法给初始的M个节点分配索引1~M，之后每增加一个节点，其索引加1；WS方法则按照顺时针方向从小到大给N个节点分配索引。</p><p><strong>为DAG的边和节点赋上操作</strong></p><p>边操作：本文中假设每条边都代表着数据流，将一个tensor从一个节点送到另一个节点。</p><p>节点操作：每一个节点代表一个一个实际操作。这个操作可以细分为以下三步：</p><ul><li>Aggregation：（输入操作：聚合）输入数据(从一条或多条边)到节点通过加权求和来聚合在一起；其中权重是正向可学习的。</li><li>Transformation：（转换操作：卷积）聚合数据由定义为[relu-convolu-bn]三元组的转换处理。所有节点都使用相同类型的卷积，默认情况下为3×3可分卷积。</li><li>Distribution：（输出操作：复制）节点的输出边缘发送转换后的数据的相同副本。</li></ul><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155139.png" alt></p><p>这样设计操作带来的一些特点：</p><ol><li>Additive aggregation能够维持输入输出channel数不变，防止后面的卷积计算越来越大，来避免仅仅因为增加计算量而提高大型输入节点的重要性程度，而忽略网络连接的作用。</li><li>Transformation应具有相同数量的输出和输入通道(除非切换阶段），以确保转换后的数据可以与来自任何其他节点的数据相结合。固定通道计数之后，不管输入和输出的程度如何，都会保持每个节点的FLOPs(浮点操作)和参数计数不变。</li><li>不论输入和输出的程度如何，聚集和分布几乎没有参数(加权求和的参数数目很少)。此外，假设每条边都是无参数的，这样一来，<strong>图的FLOPs和参数数量与节点的数量大致成正比，且几乎与边的数量无关</strong>。</li></ol><p>这些属性几乎将FLOPs和参数计数与网络连接解耦，例如，在随机网络实例或不同生成器之间，FLOPs的偏差通常为±2%。这可以在不增加/减少模型复杂性的情况下比较不同的图。因此，任务性能的差异反映了连接模式的属性。</p><p><strong>设置cell的输入节点和输出节点</strong></p><p>给DAG指定唯一的输入节点（input）和唯一的输出节点(output)。DAG本身包含N个节点，那么额外指定一个输入节点与DAG中所有入度为0的节点相连，其负责将输入的图片转发出去；再额外指定一个输出节点与DAG中所有出度为0的节点相连，该节点进行均值计算后将结果输出。</p><h4 id="三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN"><a href="#三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN" class="headerlink" title="三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN"></a><strong>三、将多个NN堆叠起来，形成最终的随机连接神经网络RandWireNN</strong></h4><p>借鉴其他经典深度学习神经网络，作者将多个NN堆叠起来形成最终的随机连接神经网络RandWireNN。图3为一个示例，其包含共5个stages，其中stage1是一个卷积层，stage2可以是一个卷积层或者是一个NN，stage3、stage4和stage5均为NN。不同stage之间：卷积操作的stride为2，故feature map的大小逐渐减半；卷积操作的卷积核的数量x2，故feature map的通道数(即C)也x2。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/006tNc79ly1g36yof1qjrj30hx0puwo8.jpg" alt></p><p>其中每个NN的节点数N（不包含输入和输出节点）设置为32，通道数C设置为78或者109/154。不同的（N,C）对应不同的网络复杂度（FLOPs），这样可以跟不同规模（small/regular/larger）的其他经典网络进行实验比较。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155551.png" alt></p><h4 id="四、在ImageNet-1000-class任务上验证RandWireNN的表现"><a href="#四、在ImageNet-1000-class任务上验证RandWireNN的表现" class="headerlink" title="四、在ImageNet 1000-class任务上验证RandWireNN的表现"></a><strong>四、在ImageNet 1000-class任务上验证RandWireNN的表现</strong></h4><p>（1）三个随机图生成方法（ER，BA，WS）的比较</p><p>如图4所示，WS方法的表现最好，于是接下来作者挑选了WS（4,0.75）与其他网络进行比较。论文还进行了另外一个网络结构鲁棒性测试的实验，即将网络中的某个节点或者某条边随机去掉，然后测试网络的表现，最后发现WS的结构最不稳定。这个研究点很有意思，也许将来会有一种网络生成器能够生成又好又鲁棒的网络结构，类似大脑，也许有种大脑结构最不容易得精神疾病。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155705.png" alt></p><p>（2）WS（4,0.75）与其他网络的比较</p><p>首先是与小规模网络(smaller)进行比较，此时N=32，C=78。如表1所示，比MobileNet/ShuffleNet/NASNet/PNAS/DARTS等表现好，比Amoeba-C略差。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155743.png" alt></p><p>其次与中等规模的网络（regular）进行比较，此时N=32，C=109/154，如表2所示，比ResNet-50/101和ResNeXt-50/101表现好。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155913.png" alt></p><p>最后与大规模网络（larger）进行比较，此时直接使用刚才训练好的中等规模网络，不过输入图像的size由224x224提高到320x320，如表3所示，虽然比其他网络表现略差，但是计算量少了很多。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521155941.png" alt></p><p>（3）迁移表现</p><p>论文将RandWireNN作为骨干网络，用Faster R-CNN with FPN来检测目标（COCO object detection），如表4所示，比ResNet-50和ResNeXt-50效果好。</p><p><img src="https://raw.githubusercontent.com/JiaoYuZhang/picRep/master/img/20190521160014.png" alt></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Elsken, T., Metzen, J. H., &amp; Hutter, F. (2018). <em>Neural Architecture Search: A Survey</em>. <em>20</em>, 1–21. Retrieved from <a href="http://arxiv.org/abs/1808.05377" target="_blank" rel="noopener">http://arxiv.org/abs/1808.05377</a></p><p>[2] Zoph, B., &amp; Le, Q. V. (2016). <em>Neural Architecture Search with Reinforcement Learning</em>. 1–16. Retrieved from <a href="http://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">http://arxiv.org/abs/1611.01578</a></p><p>[3] Zoph, B., Vasudevan, V., Shlens, J., &amp; Le, Q. V. (2018). Learning Transferable Architectures for Scalable Image Recognition. <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 8697–8710. <a href="https://doi.org/10.1109/CVPR.2018.00907" target="_blank" rel="noopener">https://doi.org/10.1109/CVPR.2018.00907</a></p><p>[4] Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., &amp; Dean, J. (2018). <em>Efficient Neural Architecture Search via Parameter Sharing</em>. Retrieved from <a href="http://arxiv.org/abs/1802.03268" target="_blank" rel="noopener">http://arxiv.org/abs/1802.03268</a></p><p>[5] Gordon, A., Eban, E., Nachum, O., Chen, B., Wu, H., Yang, T. J., &amp; Choi, E. (2018). MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks. <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, (1), 1586–1595. <a href="https://doi.org/10.1109/CVPR.2018.00171" target="_blank" rel="noopener">https://doi.org/10.1109/CVPR.2018.00171</a></p><p>[6] Liu, H., Simonyan, K., &amp; Yang, Y. (2018). <em>DARTS: Differentiable Architecture Search</em>. Retrieved from <a href="http://arxiv.org/abs/1806.09055" target="_blank" rel="noopener">http://arxiv.org/abs/1806.09055</a></p><p>[7] Xie, S., Kirillov, A., Girshick, R., &amp; He, K. (2019). <em>Exploring Randomly Wired Neural Networks for Image Recognition</em>. Retrieved from <a href="http://arxiv.org/abs/1904.01569" target="_blank" rel="noopener">http://arxiv.org/abs/1904.01569</a></p>]]></content>
    
    <summary type="html">
    
      对于目前的NAS领域进行一个总结，各种算法都采用了哪些策略来实现搜索空间、搜索策略和评估策略三个方面。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/tags/NAS/"/>
    
      <category term="Search Space" scheme="https://jiaoyuzhang.github.io/tags/Search-Space/"/>
    
      <category term="Search Strategy" scheme="https://jiaoyuzhang.github.io/tags/Search-Strategy/"/>
    
      <category term="Performance Estimation Strategy" scheme="https://jiaoyuzhang.github.io/tags/Performance-Estimation-Strategy/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-NAS</title>
    <link href="https://jiaoyuzhang.github.io/2019/05/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-NAS/"/>
    <id>https://jiaoyuzhang.github.io/2019/05/12/论文阅读-NAS/</id>
    <published>2019-05-12T05:24:11.000Z</published>
    <updated>2019-05-16T14:43:21.017Z</updated>
    
    <content type="html"><![CDATA[<p>论文： Neural Architecture Search with Reinforcement Learning 【<a href="https://arxiv.org/abs/1611.01578" target="_blank" rel="noopener">pdf</a>】</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zoph%2C+B" target="_blank" rel="noopener">Barret Zoph</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q+V" target="_blank" rel="noopener">Quoc V. Le</a></p><p>ICLR2017</p><p>本文是NSANet的前置，比较详细的介绍了Google的NAS的整体设计，包括具体的目标函数以及实现skip connection的具体方法，主要是提出了用强化学习的方法来完成搜索神经网络架构。</p><h2 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h2><p>通过一个controllerRNN在搜索空间（search space）中得到一个网络结构（论文中称为child network），然后用这个网络结构在数据集上训练，在验证集上测试得到准确率R，再将这个准确率回传给controller，controller继续优化得到另一个网络结构，如此反复进行直到得到最佳的结果，整个过程称为Neural Architecture Search。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g30ui1oqbxj30g308cdgd.jpg" alt></p><p>正如上图，左侧为控制器，采用的是循环神经网络——具体来说是LSTM结构，采用了强化学习的Policy Gradient方式作为更新策略，每次以概率p采样一个网络结构A。采用LSTM作为控制器的最大原因是可以在可变长度的空间内进行架构搜索。</p><p>右侧为子网络（采样架构A），它将在训练集上从头开始训练到收敛，然后在验证集上验证得到准确率R。R作为Reward来衡量p此时的梯度，反馈回控制器进行更新。</p><h2 id="控制器RNN如何生成网络架构"><a href="#控制器RNN如何生成网络架构" class="headerlink" title="控制器RNN如何生成网络架构"></a>控制器RNN如何生成网络架构</h2><p>控制器生成的是网络架构中的超参数，NAS中假定预测的inference中只含卷积层。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g337uq96gtj30dt051mxn.jpg" alt></p><p>对于Layer N，控制器可以预测该层的filter width，filter height，stride height，stride width，filter的个数，每一个预测都是通过softmax分类，并作为下一个预测的输入。实验中的停止条件是，层的个数超过了某个指定的数值。当然这个数值在训练的过程中也可以变化。</p><p>最终得到的是LSTM产生的一个序列化的tokens，记为$\theta_{\mathrm{c}}$。</p><h2 id="控制器的更新策略"><a href="#控制器的更新策略" class="headerlink" title="控制器的更新策略"></a>控制器的更新策略</h2><p>控制器采用的是强化学习的Policy Gradient，来进行更新。由于我之前没接触过强化学习，这里插一段Policy Gradient的基本原理，若了解可直接跳过。</p><h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>策略梯度(Policy Gradient)，最直白的想法是：如果一个动作得到的reward多，那么我们就使其出现的概率增加，如果一个动作得到的reward少，我们就使其出现的概率减小。基于此构造出损失函数(<a href="https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution" target="_blank" rel="noopener">合理性数学证明</a>）：</p><script type="math/tex; mode=display">L(\theta)=\sum \log \pi(a | s, \theta) f(s, a)</script><p>上式中$\log \pi(a | s, \theta)$表示在状态 $s$ 对所选动作 $a$的概率。而$f(s, a)$代表的是当前状态$s$下采取动作$a$所能得到的奖励，这是当前的奖励和未来奖励的贴现值的求和。也就是说，我们的策略梯度算法必须要完成一个完整的eposide才可以进行参数更新。</p><p>核心思想是更新参数时有两个考虑：如果这个回合选择某一动作，下一回合选择该动作的概率大一些，然后再看奖惩值，如果奖惩是正的，那么会放大这个动作的概率，如果奖惩是负的，就会减小该动作的概率。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g33iv9ug57j30hu07sdi6.jpg" alt></p><h3 id="NAS的更新策略"><a href="#NAS的更新策略" class="headerlink" title="NAS的更新策略"></a>NAS的更新策略</h3><p>将NAS与强化学习做一个对应，那么控制器LSTM就是Agent，控制器预测产生$\theta_{\mathrm{c}}$对应为活动$a_{1 : T}$，子网络在验证集上的准确率R作为reward signal。</p><p>那么，就得到了目标函数：</p><script type="math/tex; mode=display">J\left(\theta_{c}\right)=E_{P\left(a_{1 : T} ; \theta_{c}\right)}[R]</script><p>鉴于R是不连续的，我们采用policy gradient来迭代更新$\theta_{\mathrm{c}}$，具体采用了REINFORCE规则：</p><script type="math/tex; mode=display">\nabla_{\theta_{c}} J\left(\theta_{c}\right)=\sum_{t=1}^{T} E_{P\left(a_{1 : T} ; \theta_{c}\right)}\left[\nabla_{\theta_{c}} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R\right]</script><p>对上述公式作一个先验近似：</p><script type="math/tex; mode=display">\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla \theta_{c} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right) R_{k}</script><p>上述公式中m为controller中一个batch的大小，也就是一个batch中的不同架构数。T表示一个网络架构中超参个数。$R_{k}$是第k个网络架构的准确率。</p><p>上述是梯度的一种无偏估计，但是方差很高。为减小方差，添加了一个baseline：</p><script type="math/tex; mode=display">\frac{1}{m} \sum_{k=1}^{m} \sum_{t=1}^{T} \nabla_{\theta_{c}} \log P\left(a_{t} | a_{(t-1) : 1} ; \theta_{c}\right)\left(R_{k}-b\right)</script><p>由于baseline fuction b不依赖与当下action，本式依然是一个无偏估计。b采用的之前架构准确率的指数加权平均（exponential moving average）。</p><p><strong>指数加权平均</strong>，可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。变量$v$在$t$时刻记为$v_t$，$\theta_{t}$为变量$v$在$t$时刻的取值，即在不使用滑动平均模型时有$v_{t}=\theta_{t}$，在使用滑动平均模型后，$v_t$的更新公式如下：</p><script type="math/tex; mode=display">v_{t}=\beta * v_{t-1}+(1-\beta) * \theta_{t}</script><p>上式中，$\beta \in[0,1)$。$\beta = 0$相当于没有使用滑动平均。</p><p>再加入Bias Correction，有：</p><script type="math/tex; mode=display">v_{t}=\frac{\beta * v_{t-1}+(1-\beta) * \theta_{t}}{1-\beta^{t}}</script><p>当$\beta$越大时，滑动平均得到的值越和θθ的历史值相关。</p><h3 id="NAS的分布式训练"><a href="#NAS的分布式训练" class="headerlink" title="NAS的分布式训练"></a>NAS的分布式训练</h3><p>其中 parameter server共同保存了控制器的所有参数，这些server将参数分发给controller，每一个controller使用得到的参数进行模型的构建，这里由于得到的参数可能不同，构建模型的策略是随机的，导致每次构建的网络结构也会不同。</p><p>每个controller会构建一个batch，也就是m个网络，然后并行地训练这些网络，得到它们的accuracy。也就是说，每一个controller会得到一个batch也就是m个网络，和它们的accuracy，然后根据之前提到的公式，计算参数的梯度。</p><p>接着，计算完梯度的controller会将梯度发送给servers。这些server在得到梯度后，分别对自己负责的参数进行更新。更新后，当controller再次训练时，会得到更新后的参数。这里如果每个controller各自发送自己的梯度，之间不进行同步，就是异步更新。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g33h7ocj05j30gl04c0t6.jpg" alt></p><h2 id="添加跳跃连接和其他操作"><a href="#添加跳跃连接和其他操作" class="headerlink" title="添加跳跃连接和其他操作"></a>添加跳跃连接和其他操作</h2><p>为了使搜索空间更为灵活，NAS采用一种set-selection型的attention机制添加skip connection。具体来说，通过添加在第N层添加N-1个anchor，来确定是否要与之前的某一层跳跃连接。anchor通过两层（本层和上一层）的hidden state，用sigmoid来判断：</p><script type="math/tex; mode=display">\mathrm{P}(\text { Layer } \mathrm{j} \text { is an input to layer i })=\operatorname{sigmoid}\left(v^{\mathrm{T}} \tanh \left(W_{\text {prev}} * h_{j}+W_{\text {curr}} * h_{i}\right)\right)</script><p>公式中的$W_prev$、$W_curr$和$v$是可训练变量。这些行为依然是概率分布的，因此REINFORCE方法不会有大的改动。</p><p>由下图可以形象的了解anchor point是怎样添加跳跃连接的。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g33hucyawzj30f4068dgi.jpg" alt></p><p>关于添加skip connections后会出现的不统一问题，如输入的大小不一，或者某些层根本没有输入或者输出，处理方法有：</p><ol><li>如果一个层没有输入，那么就把图片作为输入</li><li>最后一层，把所有没有被链接的层的输出concat起来。然后输入分类器。</li><li>如果两个concat的层的大小不一致，则小的补0。</li></ol><p>同样用anchor，可以添加其他操作，像pooling、loacl contrast normalization等，每一个操作在RNN中多添加一个节点。 </p><h2 id="训练一个RNN网络"><a href="#训练一个RNN网络" class="headerlink" title="训练一个RNN网络"></a>训练一个RNN网络</h2><p>把递归单元看做成一棵树，$x_t$和$h_{t-1}$是输入，$h_t$是输出。控制器需要预测树的每个节点用什么链接方式：addition, elementwise multiplication等，或者激活函数：tanh, sigmoid等。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g33i4sziabj30jb07uabu.jpg" alt></p><p>具体步骤参看原论文吧，讲的比较清楚。</p><p>以上为一个2个叶节点的树，实际实验中采用了8个叶节点。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>本篇的实验结果并无太多参考意义，因为很快作者就出了NASNet。NAS最大的意义还是在提出了一种神经网络搜索的框架结构。</p><p>实验中采用了2层35个隐藏单元的LSTM，使用800个GPU训练了28天，一共22400 GPU-Hours，一共训练了12800个架构。</p><p>在生成NAS-CNN的实验中，使用的是CIFAR-10数据集。网络中加入了BN和跳跃连接。卷积核的高的范围是[1, 3, 5, 7]，宽的范围也是[1, 3, 5, 7]，个数的范围是[24, 36, 48, 64]。步长分为固定为1和在 [1, 2, 3] 中两种情况。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g33icc5gblj30k40fu77q.jpg" alt></p><blockquote><p>对于CNN的生成，论文附录贴了一个这样的图，下图中的网络在精度和深度之间达到不错的平衡。有意思的是，网络有不少滤波器是矩形的，而不是我们常用的正方形。而，更有意思的是，层越深，网络偏向于用更大的卷积核。</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g33ieuvnixj30g70o4gpp.jpg" alt></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://www.jianshu.com/p/3192f157d067" target="_blank" rel="noopener">https://www.jianshu.com/p/3192f157d067</a></li><li>Policy Gradient：<a href="https://www.jianshu.com/p/2ccbab48414b" target="_blank" rel="noopener">https://www.jianshu.com/p/2ccbab48414b</a></li><li>Policy Gradient：<a href="https://zhuanlan.zhihu.com/p/21725498" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21725498</a></li><li>指数加权平均：<a href="https://www.cnblogs.com/wuliytTaotao/p/9479958.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuliytTaotao/p/9479958.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      本文是NSANet的前置，比较详细的介绍了Google的NAS的整体设计，包括具体的目标函数以及实现skip connection的具体方法，主要是提出了用强化学习的方法来完成搜索神经网络架构。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="Google Brain" scheme="https://jiaoyuzhang.github.io/tags/Google-Brain/"/>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>matlab函数转python</title>
    <link href="https://jiaoyuzhang.github.io/2019/04/16/%E5%9B%BE%E5%83%8F%20matlab%20%E8%BD%AC%20python%20%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%82%B9/"/>
    <id>https://jiaoyuzhang.github.io/2019/04/16/图像 matlab 转 python 的一些小点/</id>
    <published>2019-04-16T09:02:31.000Z</published>
    <updated>2019-04-16T13:01:05.446Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>matlab <code>imfilter</code> </p><p>对应为python中的<code>scipy.ndimage.filters.convolve</code></p><p>具体对应</p><p>matlab：</p><p><code>res = imfilter(X, K);</code></p><p>python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.ndimage</span><br><span class="line">res = scipy.ndimage.filters.convolve(X, K, mode=<span class="string">'constant'</span>)</span><br></pre></td></tr></table></figure></li></ol><p>另，<code>imfilter</code>的’replicate’ 对应的是<code>scipy.ndimage.filters.convolve</code>的’nearest’</p><ol><li>matlab 与numpy 不同的运算符对照</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">matlab</th><th style="text-align:center">numpy</th></tr></thead><tbody><tr><td style="text-align:center">.*</td><td style="text-align:center">*</td></tr><tr><td style="text-align:center">./</td><td style="text-align:center">/</td></tr><tr><td style="text-align:center">^ (.^)</td><td style="text-align:center">**</td></tr></tbody></table></div><ol><li><p>matlab <code>a:c:b</code></p><p>对应的是numpy中的</p><p><code>np.range(a,b,c)</code></p><p>需要注意的是b这里是界限，不会被包括进去，需要适当扩大范围。</p></li><li><p>Matlab <code>logical</code></p></li></ol><p>对应为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test = np.array([<span class="number">1</span>, <span class="number">2</span> , <span class="number">0</span> , <span class="number">2.5</span> , <span class="number">-1.0</span>])</span><br><span class="line">test = test.astype(bool)</span><br><span class="line">test = test.astype(int)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      想要把matlab的一个图像处理函数转到python下。遇到的一些对应函数处理。
    
    </summary>
    
      <category term="coding" scheme="https://jiaoyuzhang.github.io/categories/coding/"/>
    
    
      <category term="图像处理" scheme="https://jiaoyuzhang.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="coding" scheme="https://jiaoyuzhang.github.io/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-ShuffleNetV2</title>
    <link href="https://jiaoyuzhang.github.io/2019/04/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNetV2/"/>
    <id>https://jiaoyuzhang.github.io/2019/04/08/论文阅读-ShuffleNetV2/</id>
    <published>2019-04-08T02:39:10.000Z</published>
    <updated>2019-04-08T03:11:49.970Z</updated>
    
    <content type="html"><![CDATA[<p>论文：ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design【<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ningning_Light-weight_CNN_Architecture_ECCV_2018_paper.pdf" target="_blank" rel="noopener">dpf</a>】</p><p>作者：<strong>Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun</strong>; </p><p>2018.7</p><h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p>轻量化模型比较好的几个模型：Xception [12], MobileNet [13], MobileNet V2 [14], ShuffleNet [15], and CondenseNet [16]。</p><p>关于衡量标准，通常使用的是非直接标准FLOPS（浮点操作数），但是FLOPS并不一定决定直接标准speed和latency。比如MobileNetV2和NASNET-A有相似的FLOPS，但是MobileNetV2速度更快。具体的差距在图1中。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g1uz7f50v3j313k0tgn5o.jpg" alt></p><p>造成FLOPS与speed不同步的主要原因有两个：</p><p>1.没考虑一些影响速度的额外因素，memory access cost（MAC）和并行度。MAC是算力强大的GPU的瓶颈。并行化程度越深，相同FLOPS的运算速度越快。</p><p>2.相同FLOPS的操作在不同的平台的运行时间是不同的。比如，张量分解在最近的一些工作中用来分解矩阵乘法，但是最近研究表明分解操作虽然减少了75%的FLOPS但会在GPU上更慢。我们调查了这一现象，发现原因是最新的CUDNN库为3<em>3卷积特别进行了优化。我们不能再简单认为3</em>3卷积就是比1*1卷积慢了9倍。</p><p>基于以上的观察，本文认为在设计效率模型时必须考虑两个原则：1.采用直接标准取代FLOPS 2.应该在同一平台衡量直接标准。</p><h2 id="二-效率网络设计的实践指导"><a href="#二-效率网络设计的实践指导" class="headerlink" title="二 效率网络设计的实践指导"></a>二 效率网络设计的实践指导</h2><p>本文的研究都是基于两个广泛使用的平台，使用的是工业级优化过的CNN库。本文的CNN库是比其他开源CNN库更高效的。</p><p>GPU平台使用的是一台1080Ti，卷积库使用CUDNN7.0。我们也激活了SUDNN的benchmarking功能来选择不同卷积操作的最快实现算法。</p><p>ARM平台使用一块高通骁龙810。本文采用了基于NEON的高优化实现方案。测试集使用单线程。</p><p>其他设置包括：开启全优化操作（使用tensor fusion）。输入图像大小224*224.每个网络都是任意初始化，测试了100次，采用平均时间。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g1uz8k3px4j314i0fu13l.jpg" alt></p><p>在图2中，本文比较了ShuffleNetV1和MobileNetV2在不同平台上的操作时间占比其实是不同的。FLOPS只考虑了卷积操作部分。虽然卷积操作占比为主要，但是数据传输、数据打乱和元素级操作（张量加、ReLU等）也占据了相当比分的时间。</p><p><strong>G1) Equal channel width minimizes memory access cost (MAC).</strong>相等的通道宽度能最小化内存访问代价。现代网络结构通常会采用深度可分卷积，其中点卷积构成了最大的复杂度。仔细研究点卷积的核的形状。它由两个参数决定：输入通道数$c_1$和输出通道数$c_2$。$h$和$w$表示特征图的空间尺寸，那么点卷积的FLOPS就为$B=hwc_1c_2$.</p><p>为了简化说明，这里假设计算设备的cache足够大到存储整个feature maps和参数。那么，访存操作数就是$MAC = hw（c_1+c_2）+c_1c_2$.两个变量特别相关于访存，是input/output feature maps and kernel weights。对应不等式：</p><script type="math/tex; mode=display">\mathrm{MAC} \geq 2 \sqrt{h w B}+\frac{B}{h w}</script><p>由此可见，MAC有FLOPS一个很低的关联，当输入输出通道数相等时它的相关度会更低。</p><p>这是一个理论化的结果。实践中，很多设备的闪存根本不够大。还有其他计算设计问题，因此我们直接设计了一个对比试验。一个网络结构由十个构建块堆叠成。每块包含两个卷积层。在保证FLOPS的前提下，我们调整$c_1$与$c_2$的比例，结果如表1.</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1g1uzdt1226j314o0cejuw.jpg" alt></p><p><strong>G2) Excessive group convolution increases MAC.</strong>过多的组卷积会增加MAC。群卷积是现代网络结构的核心[7,15,25,26,27,28]。它通过将所有通道之间的密集卷积变为稀疏(仅在通道组内)来降低计算复杂度(FLOPs)。一方面，它允许在给定固定故障的情况下使用更多的通道，并增加了网络容量(从而提高了准确性)。然而，另一方面，频道数量的增加导致更多的MAC。</p><p>基于G1的推导，在1*1的点卷积中的MAC与FLOPs的关系为：</p><script type="math/tex; mode=display">\begin{aligned} \mathrm{MAC} &=h w\left(c_{1}+c_{2}\right)+\frac{c_{1} c_{2}}{g} \\ &=h w c_{1}+\frac{B g}{c_{1}}+\frac{B}{h w} \end{aligned}</script><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g1v0nb82c0j314c0cegoo.jpg" alt></p><p>因此，我们建议，组数应该基于平台和任务慎重选择。仅仅只为了更多的通道数而用更大的组数是不理智的，因为准确的增长可能比不过为此付出的计算代价的增长。</p><p><strong>G3) Network fragmentation reduces degree of parallelism.</strong>网络碎片会减少并行度。</p><p>碎片化操作对于准确率有提高，但是对高并行化设备比如GPU不友好。</p><p><strong>G4) Element-wise operations are non-negligible.</strong>元素级操作不可忽略。</p><h2 id="三-ShuffleNet-V2"><a href="#三-ShuffleNet-V2" class="headerlink" title="三 ShuffleNet V2"></a>三 ShuffleNet V2</h2><p>对于ShuffleNet v1中的组点卷积和瓶颈结构都会增加MAC，而且对于轻量级模型是不可忽略的。为了改善上述问题，本文提出了Channel Split。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1g1v0of9uk2j313q0osn30.jpg" alt></p><p>也就是在每个单元的开头将输入的c个通道分割成两个分支，一个为$c-c^`$，另一个有$c^`$个通道。遵循G3，左边的分支保持了恒等。而右边的分支保证了输入输出通道数的相同满足G1。两个点卷积操作不再进行组卷积，一方面为了遵循G2，另一方面通道分割已经分出了两组。</p><p>在卷积完成后，两个分支直接进行拼接。拼接完成后，再进行channel shuffle。之后就接下一个单元，不再进行ShuffleNetV1中的ADD操作。元素操作比如ReLU和可分卷积只存在在一个分支中。另外拼接、通道洗牌、通道分割都合并为一个单一的元素操作。</p><p>为了做空间下采样，单元也被做了简单的修改如图3.d。这里得$c^`$为了方便直接设为c的一半，ShuffleNet V2的最后结构如表格5.</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1g1v0qpb07cj314g0muq8c.jpg" alt></p><p>与ShuffleNet V1整体结构唯一不同的是，在全局平均池化层之前添加了一个点卷积层来混合通道。</p><p>对于ShuffleNet V2的分析，首先高效性使得每个构建块都能使用更多的特征通道。其次，一半通道直接进入下一个构建块可以视为某种程度的feature reuse。</p><h2 id="四-实验"><a href="#四-实验" class="headerlink" title="四 实验"></a>四 实验</h2><p>在COCO数据集上，基础框架是Light-Head RCNN。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1g1v0rpffvjj314i0fujwf.jpg" alt></p><h2 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h2><p>本文建议网络架构设计应考虑速度等直接指标，而不是FLOPs等间接指标。我们提出了实用的指导方针和一个新的架构，ShuffleNet v2。综合实验验证了该模型的有效性。</p>]]></content>
    
    <summary type="html">
    
      现在的网络结构的衡量通常通过不直接的衡量标准像FLOPS等，但是直接衡量标准比如速度其实取决于很多其他原因，内存访问成本和平台因素都有很大关系。本文的工作就是直接在目标平台衡量直接标准速度，而不仅仅是考虑FLOPS。基于一系列对照实验，本文给出了一些设计效率网络的建议。本文也给出了一个新的网络结构ShuffleNetV2。完全的消融实验证实了本文的模型是截止目前最优秀的速度与准确率的结合。
    
    </summary>
    
      <category term="轻量化卷积" scheme="https://jiaoyuzhang.github.io/categories/%E8%BD%BB%E9%87%8F%E5%8C%96%E5%8D%B7%E7%A7%AF/"/>
    
    
      <category term="轻量化卷积" scheme="https://jiaoyuzhang.github.io/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="dw convolution" scheme="https://jiaoyuzhang.github.io/tags/dw-convolution/"/>
    
      <category term="group convolution" scheme="https://jiaoyuzhang.github.io/tags/group-convolution/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-darts</title>
    <link href="https://jiaoyuzhang.github.io/2019/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-darts/"/>
    <id>https://jiaoyuzhang.github.io/2019/03/29/论文阅读-darts/</id>
    <published>2019-03-29T15:06:07.000Z</published>
    <updated>2019-05-19T14:40:28.715Z</updated>
    
    <content type="html"><![CDATA[<p>DARTS：架构搜索的可微解决</p><p>论文： DARTS: Differentiable Architecture Search 【<a href="https://arxiv.org/abs/1806.09055" target="_blank" rel="noopener">pdf</a>】【 <a href="https://github.com/quark0/darts" target="_blank" rel="noopener">code</a>】</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+H" target="_blank" rel="noopener">Hanxiao Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Simonyan%2C+K" target="_blank" rel="noopener">Karen Simonyan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+Y" target="_blank" rel="noopener">Yiming Yang</a></p><h2 id="第一部分-什么是神经网络架构搜索"><a href="#第一部分-什么是神经网络架构搜索" class="headerlink" title="第一部分 什么是神经网络架构搜索"></a>第一部分 什么是神经网络架构搜索</h2><h3 id="1-1-神经网络架构搜索（Neural-Architecture-Search）"><a href="#1-1-神经网络架构搜索（Neural-Architecture-Search）" class="headerlink" title="1.1 神经网络架构搜索（Neural Architecture Search）"></a>1.1 神经网络架构搜索（Neural Architecture Search）</h3><p>深度学习模型在很多任务上都取得了不错的效果，但调参对于深度模型来说是一项非常苦难的事情，众多的超参数和网络结构参数会产生爆炸性的组合，常规的 random search 和 grid search 效率非常低，因此最近几年神经网络的架构搜索和超参数优化成为一个研究热点。<br>NAS通常由三部分组成。首先确定search space，然后选择search strategy，最后选择评价策略并迭代优化。<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1aiesn12ej313g09ytbk.jpg" alt><br>搜索空间：搜索空间定义了优化问题的变量。深度学习模型的性能是由网络架构参数和对应的超参数来决定的，因此只需要对复杂模型的架构参数和对应的超参数进行优化即可。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1aiml9o9kj30ia0i0wg5.jpg" width="250," height="250"></p><p>最开始为链式的搜索空间，然后RNN中也采用了分支搜索。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1aip9bswoj30m00kctbg.jpg" width="250," height="250"></p><p>现在，通常使用cell来对搜索空间做“降维”，能够大大提高搜索速度。    </p><p>搜索策略：定义了使用怎样的算法可以快速、准确找到最优的网络结构参数配置。常见的搜索方法包括：随机搜索、贝叶斯优化、进化算法、强化学习、基于梯度的算法。<br>评价预估：用一些低保真的训练集来训练模型，借鉴于工程优化中的代理模型，参数级别的迁移。单次（One-Shot）架构搜索。</p><h3 id="1-2-主流架构搜索-不可微的“黑箱”优化"><a href="#1-2-主流架构搜索-不可微的“黑箱”优化" class="headerlink" title="1.2 主流架构搜索-不可微的“黑箱”优化"></a>1.2 主流架构搜索-不可微的“黑箱”优化</h3><p> 国内做AutoML，主流搜索算法是进化算法或者强化算法进行不可微搜索。强化学习：NAS、ENAS， 进化算法：</p><h2 id="第二部分-DARTS架构"><a href="#第二部分-DARTS架构" class="headerlink" title="第二部分 DARTS架构"></a>第二部分 DARTS架构</h2><h3 id="2-1-darts的整体构架"><a href="#2-1-darts的整体构架" class="headerlink" title="2. 1 darts的整体构架"></a>2. 1 darts的整体构架</h3><p>首先，我们介绍一下Darts的搜索空间与整体流程。</p><p>搜索空间：在本文中以一个cell（或者叫building block）作为一个搜索空间。它是由nodes和edges构成的有向无环图。每个node被记为 <script type="math/tex">x^{(i)}</script> ，是一个特征表示（比如feature map）。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1aiyw5juhj306o0dut9c.jpg" width="200" height="250" align="center"></p><p>而每条边edge被记为<script type="math/tex">o^{(i,j)}</script>，它表示的是从节点<script type="math/tex">x^{(i)}</script>到<script type="math/tex">x^{(j)}</script>所采用的操作（比如3*3卷积，最大池化），当然也可以是特殊的无操作<script type="math/tex">zero</script>。这样我们就可以把搜索空间表示为：</p><script type="math/tex; mode=display">x^{(i)}=\sum_{j<i} o^{(i, j)}\left(x^{(j)}\right)</script><p>经过darts的优化获得最后的cell架构后，我们既可以把它堆叠成CNN网络，也可以递归连接成RNN网络。对于每个cell，我们规定它有两个输入和一个输出。对于CNN网络，两个输入是前两层的输出；对于RNN，两个输入时这一层的输入和上一层的状态。<br>以此为基础，我们可以看一下darts的整体流程：<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1aj9k3khzj31080f643i.jpg" width="600" height="250" align="center"></p><ol><li>初始化时，每条边即操作是未知的</li><li>采用松弛策略，使每条边上都进行多种候选操作</li><li>采用bilevel optimazation来联合优化操作的混合概率和网络权重</li><li>采用混合操作中概率最高的操作来替换混合操作，得到最终结构</li></ol><h3 id="2-2-优化策略与近似计算"><a href="#2-2-优化策略与近似计算" class="headerlink" title="2.2 优化策略与近似计算"></a>2.2 优化策略与近似计算</h3><p>接下来我们会详细介绍darts的整个优化策略。</p><h4 id="1-将搜索空间可微化"><a href="#1-将搜索空间可微化" class="headerlink" title="1. 将搜索空间可微化"></a>1. 将搜索空间可微化</h4><p>darts通过引入softmax来做了一个简单的松弛方案，使得整个搜索空间变为可微的。softmax的公式如图，非常简单的公式。   </p><script type="math/tex; mode=display">S_{i}=\frac{e^{i}}{\sum_{j} e^{j}}</script><p>扩展到本文中，就是这样的：</p><script type="math/tex; mode=display">\overline{O}^{(i, j)}(x)=\sum_{o \in \mathcal{O}} \frac{\exp \left(\alpha_{o}^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)} o(x)</script><p>其中<script type="math/tex">\alpha^{(i, j)}</script>是一个向量，表示的是节点$ (i, j) $中的混合操作，$\overline{o}^{(i, j)}$是混合概率操作， $ \mathcal{O} $是候选操作集合。</p><p>$\alpha$ 就是整体框架的向量表示。</p><p>现在，我们就把操作选择从不可微转化为可微操作。</p><p>我们最后在选择操作是就是选择概率最大的选项。</p><script type="math/tex; mode=display">o^{(i, j)}=\operatorname{argmax}_{o \in \mathcal{O}} \alpha_{o}^{(i, j)}</script><h4 id="2-建模为双层优化问题"><a href="#2-建模为双层优化问题" class="headerlink" title="2. 建模为双层优化问题"></a>2. 建模为双层优化问题</h4><p>先定义两个loss： $L_{train}$和$L_{val}$。那么很显然，我们的最终目标是找到一个$\alpha$ 使得$L_{val}$最小，但是需要注意的是$L_{val}$不仅有关于$\alpha$，还有关于$w$。</p><p>我们就可以得到目标函数：</p><script type="math/tex; mode=display">\min _{\alpha} \mathcal{L}_{v a l}\left(w, \alpha\right)</script><p>但是必须注意的是，$w$是相关于$\alpha$的，架构$\alpha$的变化会带来$w$的改变，所以目标函数要变成：</p><script type="math/tex; mode=display">\min _{\alpha} \mathcal{L}_{v a l}\left(w^{*}(\alpha), \alpha\right)</script><p>而$w$与$\alpha$的真正关系是怎样的呢？其实是$w$的选取，其实是相对于固定架构$\alpha$，取得最小的$L_{train}$时的$w^{*}$，也就是说，关于$w^{*}$选取的目标函数是：</p><script type="math/tex; mode=display">w^{*}(\alpha)=\operatorname{argmin}_{w}  \mathcal{L}_{t r a i n}(w, \alpha)</script><p>现在，我们得到了最后的优化函数，它是一个双层优化问题：</p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{\alpha}} & {\mathcal{L}_{v a l}\left(w^{*}(\alpha), \alpha\right)} \\ {\text { s.t. }} & {w^{*}(\alpha)=\operatorname{argmin}_{w} \mathcal{L}_{\text {train}}(w, \alpha)}\end{array}</script><p>（不太好理解的话，架构$\alpha$这个参数的地位有点类似于lr学习率，但是显然比简单线性增减的学习率复杂太多）。</p><p>因为下层子问题的自变量$w$与上层问题的$L_{val}$无法直接用表达式表达出来，所以这个问题无法被简化为单层优化问题。双层问题的求解是非常复杂的，$\alpha$的每一次变化，$w$就需要重新求解一次。整个问题的时间复杂度可以达到$\mathrm{O}(|\mathrm{a}||\mathrm{w}|)$。</p><p>而如何能够在实际中解决这个问题，我们需要采用一些近似优化的方法。</p><h4 id="3-近似迭代优化：梯度下降"><a href="#3-近似迭代优化：梯度下降" class="headerlink" title="3. 近似迭代优化：梯度下降"></a>3. 近似迭代优化：梯度下降</h4><p>darts中采用了近似迭代的方法来解决这个问题，通过$w$和$\alpha$分别在权重和构架空间中的梯度下降步骤之间交替来完成优化。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1k697frydj31o60e2tph.jpg" alt></p><p>我们来看这个优化算法，在第k步的时候，首先采用$\alpha{k-1}$的架构，通过$ \mathcal{L}_{t r a i n}\left(w_{k-1}, \alpha_{k-1}\right) $获得现在的权重$ w{k}$，利用梯度更新一步$ w{k}$， 再固定$ w{k}$，进行主问题的优化：</p><script type="math/tex; mode=display">\mathcal{L}_{v a l}\left(w^{\prime}, a_{k-1}\right)=\mathcal{L}_{v a l}\left(w_{k}-\xi \nabla_{w} \mathcal{L}_{t r a i n}\left(w_{k}, \alpha_{k-1}\right), a_{k-1}\right)</script><p>其中，$\xi $是$ w$的学习率。$w_{k}-\xi \nabla_{w} \mathcal{L}_{t r a i n}\left(w_{k}, \alpha_{k-1}\right)$这一项就是表示$ w{k}$的一步梯度更新。</p><p>这个式子的背后动机是，寻找一个架构$\alpha{k}$使得权重$ w{k}$在进行单步梯度下降后$L_{val}$会更低。这种优化方式采用的是Stackelberg game，但是没有收敛担保，只有实践证明。同时我们发现采用momentum梯度更新，我们的推论依旧是成立的。</p><p>由上面的推断，我们可以得到构架a的梯度表示：</p><script type="math/tex; mode=display">\nabla_{\alpha} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)-\xi \nabla_{\alpha, w}^{2} \mathcal{L}_{t r a i n}(w, \alpha) \nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)</script><p>可以注意到第二项中有一个二次梯度，这一项的计算是极为复杂的，所幸微分是可以有近似操作的。</p><script type="math/tex; mode=display">\nabla_{\alpha, w}^{2} \mathcal{L}_{t r a i n}(w, \alpha) \nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right) \approx \frac{\nabla_{\alpha} \mathcal{L}_{t r a i n}\left(w^{+}, \alpha\right)-\nabla_{\alpha} \mathcal{L}_{t r a i n}\left(w^{-}, \alpha\right)}{2 \epsilon}</script><p>其中，有$w^{+}=w+\epsilon \nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)$和$w^{-}=w-\epsilon \nabla_{w^{\prime}} \mathcal{L}_{v a l}\left(w^{\prime}, \alpha\right)$。</p><p>计算有限差分只需要两次权值前传和两次向后传递，复杂度从$\mathrm{O}(|\mathrm{a}||\mathrm{w}|)$降低为$\mathrm{O}(|\mathrm{a}|+|\mathrm{w}|)$。</p><p>可以考虑一个特殊情况，学习率为0的时候，上式就变成了一个一阶近似。而$\alpha$的梯度则完全取决于前一项。这样的话，$\alpha$和$w$就相对独立了，但是这种情况下虽然速度有了提高，效果却下降的厉害。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1k6lwncmvj31m80u0hdt.jpg" alt></p><h4 id="4-还原为离散网络结构"><a href="#4-还原为离散网络结构" class="headerlink" title="4. 还原为离散网络结构"></a>4. 还原为离散网络结构</h4><p>最后得到了架构的参数a，要将它还原为离散的网络构架，我们进行两步操作：</p><ol><li><p>为每个节点选取最强的k个预处理，这个k的选取依据以下公式。为了和其他网络结构保持可对比性，我们的卷积单元k=2，循环单元k=1。</p><script type="math/tex; mode=display">\max _{o \in \mathcal{O}, o \neq z e r o} \frac{\exp \left(\alpha_{o}^{(i, j)}\right)}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left(\alpha_{o^{\prime}}^{(i, j)}\right)}</script></li><li><p>以argmax取代所有混合操作，成为最有可能的操作。</p></li></ol><p>以下为两个训练出来的结果：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1k6p1emunj31240ki7jt.jpg" width="250," height="250"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1k6ooqv18j30oq0w2als.jpg" width="250," height="250"></p><h3 id="2-3-实验结果"><a href="#2-3-实验结果" class="headerlink" title="2.3 实验结果"></a>2.3 实验结果</h3><p>在这部分，具体的实验细节就不提供了，本文中在CIFAR-10数据集上进行CNN的cell训练。对于CNN网络，darts一共训练两种cell：normal cell和reduction cell。区别在于stride，前者为1，后者为2后再网络三分之一处和三分之二处。在Penn Treebank上训练RNN cell。只有一种单元。如下图显示，整个构架搜索是初始化敏感的，CNN cell经过长时间训练后能得到比较好的收敛，但是RNN cell的结果与初始化关系比较密切。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1k6ung22xj30xw0dkk5b.jpg" alt></p><p>第二部分的评估，是将训练好的cell迁移到ImageNet或者WikiText-2上进行测试。在ImageNet上的效果是比较好的，而WikiText-2上的效果一般，可能原因是文本数据集差异过大。</p><h2 id="第三部分-疑问"><a href="#第三部分-疑问" class="headerlink" title="第三部分 疑问"></a>第三部分 疑问</h2><ol><li>为什么权值更新采用momentum梯度更新法？</li></ol>]]></content>
    
    <summary type="html">
    
      利用梯度下降进行神经网络架构搜索，和之前的利用强化或者进化来做nas的效率有很大改进。
    
    </summary>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/categories/NAS/"/>
    
    
      <category term="轻量化卷积" scheme="https://jiaoyuzhang.github.io/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="NAS" scheme="https://jiaoyuzhang.github.io/tags/NAS/"/>
    
      <category term="darts" scheme="https://jiaoyuzhang.github.io/tags/darts/"/>
    
      <category term="autoML" scheme="https://jiaoyuzhang.github.io/tags/autoML/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读- ShuffleNet</title>
    <link href="https://jiaoyuzhang.github.io/2019/03/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ShuffleNet/"/>
    <id>https://jiaoyuzhang.github.io/2019/03/27/论文阅读-ShuffleNet/</id>
    <published>2019-03-27T13:33:49.000Z</published>
    <updated>2019-03-27T13:39:24.460Z</updated>
    
    <content type="html"><![CDATA[<p>论文标题：ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices 【<a href="https://arxiv.org/pdf/1707.01083.pdf" target="_blank" rel="noopener">pdf</a>】</p><p>作者：Xiangyu Zhang∗ Xinyu Zhou∗ Mengxiao Lin Jian Sun</p><p>旷视的轻量化卷积网络。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>ShuffleNet：能够应用于移动场景的轻量级应用。它的操作限制在10-150MFLOPs。</p><p>这个新结构采用了两种新结构：pointwise group convolution（点群卷积）和channel shuffle（打乱通道）。</p><p>实验采用了ImageNet分类问题和COCO上的目标检测问题，在算力限制40MFLOPs它相比于mobilenetv1在分类问题上有更低的top-1错误率（7.8%）。在ARM-based移动设备上，它和AlexNet准确率可比较的同时速度快上13倍。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>现阶段在解决视觉识别任务时构建更深更大的卷积网络是大势。本文则追求在有限运算次数中追求更高准确率。现有的轻量级模型聚焦在剪枝（pruning）、挤压（compressing）或者低位（low-bit）一个基础神经网络。</p><p>本文注意到标准基础结构比如Xception、ResNeXt由于1<em>1卷积层的巨大消耗在小型网络中表现低效。本文采用pointwise group convolution（PGC）来减少1</em>1卷积层的计算复杂性。为了克服PGC带来的边际效应，本文使用channel shulffle来得到信息在特征层之间流动。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>有效的模型设计。GoogleNet在更少复杂性下加深了网络深度。SqueezeNet在准确率维持的同时减少了参数和计算。ResNet使用了bottleneck结构。SENet引入了一个表现良好的轻量级计算的结构单元。</p><p><strong>Group Convolution。</strong>组卷积的概念一开始是AlexNet为了在两个GPU上计算而提出的，在ResNeXt中显示出了它的效率。Depthwise separable convolution在Xception中被使用。最近的MobileNet使用可分卷积在轻量级模型上取得了最先进的结果。本文以全新的形式使用了组卷积和可分卷积。</p><p><strong>Channel Shuffle Operation。</strong>据我们所知，通道洗牌操作之前很少在效率模型中被使用。CNN库cuda-convnet中支持“任意可分卷积”层，在组卷积中等同于任意通道洗牌。但是这样的任意洗牌操作的目的不同，后来也很少被用到。最近，另一个网络也在两阶段卷积中采用了这一创意，但是它没有发现通道洗牌的作用，也没有把它用于小型网络设计。</p><p><strong>模型加速。</strong>模型加速是要在准确保持的情况下加速前向网络（inference）。剪枝网络在保持表现的同时减去了预训练模型的冗余链接。量化和因式分解的目的都是减去冗余计算来加速前向网络。实践中还有利用FFT加速CNN卷积计算。另外，Distilling（蒸馏）将大模型的信息提取到小模型中，使得训练小模型更为简单。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="组卷积中使用通道洗牌"><a href="#组卷积中使用通道洗牌" class="headerlink" title="组卷积中使用通道洗牌"></a>组卷积中使用通道洗牌</h3><p>​    现代卷积网络中通常由重复法人相同结构的构建块组成。其中，目前最为优秀的Xception或ResNeXt在构建块中引入可分卷积或者组卷积，以此来取得可靠能力和计算代价间的完美平衡。然而，本文 注意到它们都没有完整考虑点卷积的开销。比如，ResNeXt中只有3*3层使用了组卷积。结果是在ResNeXt的每个残差单元中点卷积占了93.4%的multiplication-adds操作。在小网络中，昂贵的点卷积导致了通道数受到复杂度的限制，这一点对准确度的降低有显著影响。</p><p>为解决这一问题，一个直接的方法是在点卷积上使用通道可分连接（比如组卷积）。通过确保每次卷积操作只作用在对应的输入通道组上，组卷积显著地减少了计算代价。但是，多次组卷积堆叠在一起会产生边际效应：某一特定通道的输出只来源于输入通道的一部分。图1.a展示了两个堆叠的租卷积层。很显然特定组的输出只相关于组内输入，它使得通道组的信息流被固定在组内，削弱了代表性。</p><p>如果我们让组卷积从不同组间获得输入信息，输入通道和输出通道就会充分相关（图1.b）。特别地，对于上一个组层产生的特征图，我们可以先在组内将通道再次分为子组，再在下一层在组中喂入不同的子组。这一步可以通过通道洗牌（图1. c）高效优雅地完成：假设一个g组的卷积层，它的输出有g<em>n个通道，我们先将输出通道reshape成（g，n）矩阵，将矩阵<em>*转置</em></em>后再拉平。注意，即便两个卷积层组数不同这一步依然有效。另外，通道洗牌是可微的，这意味着它可以被嵌入经段对端训练的网络结构中。</p><p>通道洗牌使得利用多重组卷积构建更强力的网络变为可能。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1ho2vtyw2j31ec0luq8i.jpg" alt></p><h3 id="ShuffleNet-Unit"><a href="#ShuffleNet-Unit" class="headerlink" title="ShuffleNet Unit"></a>ShuffleNet Unit</h3><p>利用通道洗牌，本文构建了特别为小型网络设计的全新的ShuffleNet单元。先介绍图2.a中的瓶颈单元设计原则。它是一个残差块（residual block）。在它的残差分支中，本文用计算经济的3<em>3可分卷积来替换3</em>3卷积。然后，本文将第一个点卷积层用点组卷积代替，再接了一个通道洗牌，这就形成了一个ShuffleNet单元（图2.b）。第二个点组卷积的作用是重新覆盖通道维度来适应残差捷径。为简化操作，第二个点组卷积后没有再接通道洗牌，结果也是可以接受的。Batch normalization和非线性化操作都相似于标准残差网络，但是在可分卷积后我们没有使用ReLU。鉴于ShuffleNet可能会和stride一起使用，我们在图2.c做了简单的两个改变：1.在捷径后加了一个3*3平均池化层2.用通道连接concat替换element-wise addition，使得扩发通道维数的操作计算更简单。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1ho49a3edj31dw0mswkf.jpg" alt></p><p>相较于ResNet和ResNeXt，我们的模型在相同配置下计算更简便。比如，输入为c<em>h</em>w，瓶颈层通道数为m，那么ResNet需要hw（2cm+9m^2）FLOPS，ResNeXt需要hw（2cm+9m^2/g）FLOPS，但是ShuffleNet需要hw（2cm/g+9m）FLOPS，g表示组数。这意味着，同样的计算预算，ShuffleNet可以使用更宽的特征图。</p><p>另外，ShuffleNet只在瓶颈层使用可分卷积。尽管可分卷积在理论上复杂度很低，本文发现在低能量移动设备很难效率实现，导致计算/内存可获得比例比其他维度操作低很多。这个缺点在Xception中也被提及。在ShuffleNet单元，我们只在瓶颈处使用可分卷积。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>整个网络结构构件在ShuffleNet单元上，在表格1中完整展示。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1ho4wt4yfj31dw0kqafi.jpg" alt></p><p>网络结构中主要包含了三个阶段的ShuffleNet单元。每个阶段的第一个构建块是步长为2的，其他参数在一层中保持不变，进入下一层时输出通道数为变为2倍。类似于ResNet，本文在每个ShuffleNet单元设置瓶颈通道数为输出通道数的1/4。本文意在提供最为简单的偏好设计，但是也发现经过调参结果会更好。</p><p>在ShuffleNet单元中，组数g控制着点卷积的稀疏度。表格1中显示了不同的g，我们调整了输出通道数来保持计算代价基本不变（~140MFLOPS）。显然，组数越大输出通道数越多（更多的卷积过滤器），也就能编码更多的信息，但是它也意味着每个单独的卷积过滤器因为相关输入通道变少而退化。</p><p>为了使网络结构达到预计的复杂度，我们简单在通道数上应用了一个尺度参数s。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>本文的实验主要在ImageNet2012上进行。实验细节在此不做赘述。</p><p>在ImageNet2012上得实验结果为：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1ho5f4ukqj310k0940v0.jpg" alt>采用通道洗牌的方式与不采用通道洗牌的不同结果：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1ho5yasjzj310q0c2add.jpg" alt><br>它与其他压缩模型的对比：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1ho6hbpczj314y0u0gxd.jpg" alt></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇论文模型压缩的主要起作用的piontwise group convolution，然后channel Shuffle提供了5%的错误率改善。</p>]]></content>
    
    <summary type="html">
    
      ShuffleNet：能够应用于移动场景的轻量级应用。它的操作限制在10-150MFLOPs。这个新结构采用了两种新结构：pointwise group convolution（点群卷积）和channel shuffle（打乱通道）。实验采用了ImageNet分类问题和COCO上的目标检测问题，在算力限制40MFLOPs它相比于mobilenetv1在分类问题上有更低的top-1错误率（7.8%）。在ARM-based移动设备上，它和AlexNet准确率可比较的同时速度快上13倍。
    
    </summary>
    
      <category term="轻量化卷积" scheme="https://jiaoyuzhang.github.io/categories/%E8%BD%BB%E9%87%8F%E5%8C%96%E5%8D%B7%E7%A7%AF/"/>
    
    
      <category term="轻量化卷积" scheme="https://jiaoyuzhang.github.io/tags/%E8%BD%BB%E9%87%8F%E5%8C%96%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="dw convolution" scheme="https://jiaoyuzhang.github.io/tags/dw-convolution/"/>
    
      <category term="group convolution" scheme="https://jiaoyuzhang.github.io/tags/group-convolution/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读-TinyFace GAN</title>
    <link href="https://jiaoyuzhang.github.io/2019/03/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-TinyGAN/"/>
    <id>https://jiaoyuzhang.github.io/2019/03/27/论文阅读-TinyGAN/</id>
    <published>2019-03-27T10:55:31.000Z</published>
    <updated>2019-03-27T12:45:24.397Z</updated>
    
    <content type="html"><![CDATA[<p>论文标题：Finding Tiny Faces in the Wild with Generative Adversarial Network 【<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Finding_Tiny_Faces_CVPR_2018_paper.pdf" target="_blank" rel="noopener">pdf</a>】</p><p>作者：Yancheng Bai， Yongqiang Zhang， Mingli Ding，Bernard Ghanem</p><p>本论文被CVPR2018会议录用，并评为oral层次。GAN自从2014年被提出，至今“有名有姓”的GAN网络已经有超过200种，这次cvpr会议上也有大量的基于GAN的论文，半监督学习的GAN在图像方面依旧大有可为。因此选取了一篇比较有质量的GAN的论文进行研读。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1hm52jgi2j31g00rku0y.jpg" alt></p><h2 id="背景与问题描述"><a href="#背景与问题描述" class="headerlink" title="背景与问题描述"></a>背景与问题描述</h2><p>人脸检测（Face Detection）是计算机视觉中的一个基本和重要的问题，是一个许多后续基于人脸的应用程序的关键一步。通常来说，人脸检测包括人脸的解析、验证以及标注和检索等。在过去的几十年里，人脸检测已被广泛研究，众多的准确有效的方法提出了使得在大多数受限的场景下人脸检测的效果都极好。最近的作品关注的是在不受控制的环境下的人脸，由于比例、模糊、姿势、表情和光照的显著变化，这是一个更具挑战性的问题。</p><p>现代人脸探测器在大中型人脸上取得了令人瞩目的效果，但在小人脸上的性能却远远不能令人满意。小脸检测通常是指例如10×10像素这一量级的人脸检测，它目前主要有两大问题：</p><p>第一，小脸由于像素过少，缺乏足够的详细信息来区分相似的背景，比如脸或手的一部分；</p><p>第二，现在的人脸检测器通常使用CNN卷积神经网络架构，采用步长为8、16或32的下采样卷积特征图来表示人脸，这些特征图丢失了大部分空间信息，过于粗糙，无法描述小人脸。</p><p>为了检测小人脸，[28]使用双线性操作直接对图像进行上采样，并对上采样图像进行详尽的人脸搜索。然而，这种方法会增加计算成本，推理时间也会大大增加。此外,图像通常与一个小升级因素放大(最多2×)[28],否则,将生成工件。此外，[1,14,25,37]利用中间的conv feature map表示特定尺度下的人脸，保持了计算量与性能之间的平衡。然而，浅层细粒度的中间conv feature map缺乏识别能力，导致了许多假阳性结果。更重要的是，这些方法不考虑其他挑战，如模糊和照明。</p><p>本篇论文做出了以下三个主要贡献：</p><ol><li>提出了一种新的统一的端到端卷积神经网络人脸检测体系结构，利用超分辨率和细化网络生成真实、清晰的高分辨率图像，并引入鉴别器网络对人脸和非人脸进行分类。</li><li>引入了一种新的丢包方法，使鉴别器网络能够同时分辨真假图像和真伪图像。更重要的是，分类损失用来引导生成网络生成更清晰的人脸，从而更容易分类。</li></ol><ol><li>最后，证明了提出的方法在从模糊的小人脸恢复清晰高分辨率人脸方面的有效性，并证明了该检测方法在较宽的人脸数据集上，特别是在最具挑战性的硬子集上，其检测性能优于其他最先进的方法。</li></ol><h2 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h2><p>本篇论文中，一共使用了三方面的技术：</p><ol><li>Face Detection</li><li>Super-resolution and Reﬁnement Network</li><li>Generative Adversarial Networks</li></ol><h3 id="FaceDetector"><a href="#FaceDetector" class="headerlink" title="FaceDetector"></a>FaceDetector</h3><p>现有的人脸检测方法大致可以分为手工制作的基于特征的方法和基于cnn的方法。</p><p>此处介绍本文中作为基准的MB-FCN。</p><p>MB-FCN：Multi-branch fully convolutional network，多分支人脸检测算法。具体做法是联合中间层feature map + 高层feature map特征，用于检测小尺度人脸，单模型的多分支检测，只需做单次前向预测即可检测图像中所有尺度的人脸，因为不用假定最小可检出人脸尺度，所以MB-FCN可以检出图像中所有尺度的人脸目标。</p><p><img src="http://note.youdao.com/src/01E3DDB023D544CA92042AEF445AE3DD" alt="img"></p><p>如fig 2，MB-FCN包含四个模块：</p><ol><li>共享的提特征主干网，本文使用ImageNet上预训练好的ResNet-50，图像经过主干网梭一把，就可以得到各层feature map啦；</li><li>为每个分支，通过feature map的skip connection操作，concate不同分辨率的feature map，用于检测不同尺度的目标，结合fig 2，就是对不同分辨率的feature map使用到了上、下采样操作，使之尺度一致后再做concate；—— 这种融合多尺度feature map，single process检测不同尺度目标的方式，计算量肯定比使用图像金字塔，做multi process检测的方式，计算量小很多；</li><li>对每个分支而言，通过FCN + multi-task学习的方式使各个分支用于特定尺度范围内的人脸；具体地，FCN先1 x 1 conv来一波以降低feature map的通道数，再接两个1 x 1 conv，输出reg、cls结果；</li><li>得到每个分支的reg、cls结果，将结果转为包含得分的bbox，全局NMS一波，即得到最终结果；</li></ol><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>在了解它的具体网络结构之前，我们先了解以下GAN。</p><p>GAN也就是对抗生成网络，它通过一个对抗的国车给训练一个生成模型G。它包含两个网络，一个生成器G和一个鉴别器D。训练过程交替优化生成器G和鉴别器D，两者相互竞争。训练生成器器G产生样本欺骗鉴</p><p>别器D，训练鉴别器D从生成器G器生成的样本和真样本中分辨真伪图像。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1hm6s5txrj30bo08f41e.jpg" alt></p><p>目标函数公式：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1hm7cnnbqj30f0018q33.jpg" alt></p><p>具体过程：</p><ul><li>G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。</li><li>D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>对于本篇论文，它的模型构建为：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1hm82ojk3j31f40ns1kx.jpg" alt></p><p>首先，图像输入网络。</p><p>第二步，以MB-FCN检测器作为基准，使用它来从输入图像中裁剪出阳性数据(即人脸)和阴性数据(即非人脸)，用于训练生成器和鉴别器，或者生成感兴趣区域(ROIs)进行测试。</p><p>第三步，训练生成器从低分辨率输入图像中重建一个4倍上采样（也就是4<em>4提高到16</em>16）高分辨率图像。其中，先使用上采样子网络（upsample sub-network），以低分辨率图像作为输入，输出为超分辨率图像。再使用细化子网络（reﬁnement sub-network），这是因为第一步生成的图像是模糊的小人脸，缺乏精细的细节，而且受MSE loss的影响，产生的超分辨率人脸通常是模糊的。</p><p>第四步，鉴别器网络为vgg19架构，具有两个并行的fc层，第一个fc层是对自然真实图像或生成的超分辨率图像进行区分，第二个fc层是对人脸或非人脸进行分类。</p><p>三、四两步交替优化。</p><p>具体的网络结构如下图：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1hm8qg49sj31dy0bkgom.jpg" alt></p><p>生成器G中，我们不输入随机噪声，而是输入模糊的小脸。它的损失函数包括三部分，一部分是像素级的损失函数，一部分是对抗损失函数，另外移除了VGG的特征匹配损失换成。</p><h2 id="思考与扩展建议"><a href="#思考与扩展建议" class="headerlink" title="思考与扩展建议"></a>思考与扩展建议</h2><p>本篇论文对于极小的人脸的检测有着比较好的实验效果，但同时，上采样的做法也导致部分非人脸的物体被误认为人脸。</p>]]></content>
    
    <summary type="html">
    
      利用GAN来做极小人脸检测
    
    </summary>
    
      <category term="GAN" scheme="https://jiaoyuzhang.github.io/categories/GAN/"/>
    
    
      <category term="GAN" scheme="https://jiaoyuzhang.github.io/tags/GAN/"/>
    
      <category term="人脸检测" scheme="https://jiaoyuzhang.github.io/tags/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
</feed>
